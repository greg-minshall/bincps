* [[https://twitter.com/kltblom][Kristian Blom]]: does recent (20 years?) change in US income distribution matter?

he [[https://twitter.com/kltblom/status/932394678241988609][showed a PDF]] (1971-2015), from Financial Times (based on data from
Pew Trust).  i don't see where to get that data.  but, [[https://www2.census.gov/programs-surveys/cps/tables/time-series/historical-income-households/h01ar.xls][Census Bureau]]
has something that breaks down by each fifth and top 5%.

#+BEGIN_SRC R :session ss :var tseries=tseries
colnames(tseries) <- c("year", "number", "lowest", "second", "third", "fourth", "llimittop5")
#+END_SRC

#+RESULTS:
| year       |
| number     |
| lowest     |
| second     |
| third      |
| fourth     |
| llimittop5 |

he has about 50 buckets.  i have 6.  sigh.

#+name: tseries
#+BEGIN_SRC sh
  xls2csv h01ar.xls |
      awk '/2016 Dollars/ { ok = 1; next} \
          /^"[12]/ {
                   if (ok) { 
                      gsub(/ *\([0-9][0-9]\) */, ""); 
                      gsub(/"/, ""); 
                      print;
                    }}' 2>&1 |
      tac
#+END_SRC

#+RESULTS: tseries
| 1967 |  60813 | 18856 | 36768 | 52186 |  74417 | 119419 |
| 1968 |  62214 | 20098 | 38103 | 54614 |  76737 | 120053 |
| 1969 |  63401 | 20699 | 39718 | 57441 |  80478 | 126218 |
| 1970 |  64778 | 20350 | 38985 | 56703 |  80899 | 127880 |
| 1971 |  66676 | 20088 | 38294 | 56353 |  80353 | 127602 |
| 1972 |  68251 | 20786 | 40033 | 59167 |  84686 | 136292 |
| 1973 |  69859 | 21238 | 40839 | 60425 |  87000 | 139832 |
| 1974 |  71163 | 21340 | 39585 | 58493 |  84892 | 134366 |
| 1975 |  72867 | 20288 | 38076 | 57536 |  82611 | 130365 |
| 1976 |  74142 | 20738 | 38636 | 58856 |  84678 | 134287 |
| 1977 |  76030 | 20694 | 38977 | 59411 |  86616 | 137142 |
| 1978 |  77330 | 21338 | 40346 | 61046 |  88785 | 142036 |
| 1979 |  80776 | 21594 | 40103 | 61700 |  89461 | 144557 |
| 1980 |  82368 | 20745 | 38905 | 59645 |  87332 | 140543 |
| 1981 |  83527 | 20340 | 38023 | 58809 |  86946 | 139925 |
| 1982 |  83918 | 20080 | 38191 | 58352 |  87015 | 143636 |
| 1983 |  85290 | 20516 | 38149 | 58550 |  88485 | 145579 |
| 1984 |  86789 | 20909 | 39134 | 60292 |  91077 | 150768 |
| 1985 |  88458 | 21154 | 39801 | 61657 |  92731 | 153220 |
| 1986 |  89479 | 21430 | 40990 | 63616 |  96164 | 161255 |
| 1987 |  91124 | 21835 | 41447 | 64697 |  97780 | 163619 |
| 1988 |  92830 | 22210 | 41953 | 65380 |  98722 | 167109 |
| 1989 |  93347 | 22614 | 43000 | 66089 | 100414 | 171533 |
| 1990 |  94312 | 22271 | 42159 | 64498 |  98359 | 168813 |
| 1991 |  95669 | 21646 | 41260 | 63729 |  97578 | 165727 |
| 1992 |  96426 | 21136 | 40494 | 63575 |  97304 | 166101 |
| 1993 |  97107 | 21217 | 40380 | 63473 |  98663 | 171210 |
| 1994 |  98990 | 21518 | 40389 | 64269 | 100717 | 176013 |
| 1995 |  99627 | 22536 | 42121 | 65734 | 101921 | 176848 |
| 1996 | 101018 | 22513 | 42318 | 67084 | 103684 | 182230 |
| 1997 | 102528 | 22979 | 43571 | 68640 | 106690 | 188834 |
| 1998 | 103874 | 23727 | 44768 | 71163 | 110418 | 194628 |
| 1999 | 106434 | 24702 | 46014 | 72630 | 114216 | 204698 |
| 2000 | 108209 | 24985 | 46009 | 72742 | 114000 | 202470 |
| 2001 | 109297 | 24361 | 45162 | 71849 | 113195 | 204021 |
| 2002 | 111278 | 23911 | 44545 | 70950 | 112127 | 200192 |
| 2003 | 112000 | 23468 | 44369 | 71059 | 113358 | 201120 |
| 2004 | 113343 | 23489 | 44059 | 70177 | 111818 | 199682 |
| 2005 | 114384 | 23570 | 44244 | 70864 | 112705 | 204014 |
| 2006 | 116011 | 23850 | 44967 | 71425 | 115508 | 207146 |
| 2007 | 116783 | 23489 | 45262 | 71770 | 115758 | 204892 |
| 2008 | 117181 | 23089 | 43476 | 69924 | 111744 | 200658 |
| 2009 | 117538 | 22880 | 43124 | 69134 | 111865 | 201359 |
| 2010 | 119927 | 22017 | 41832 | 67702 | 110116 | 198686 |
| 2011 | 121084 | 21617 | 41096 | 66609 | 108375 | 198438 |
| 2012 | 122459 | 21533 | 41568 | 67511 | 108818 | 199827 |
| 2013 | 122952 | 21535 | 41408 | 67492 | 109129 | 201957 |
| 2013 | 123931 | 21638 | 42282 | 69242 | 113582 | 211362 |
| 2014 | 124587 | 21728 | 41754 | 69153 | 113811 | 209419 |
| 2015 | 125819 | 23088 | 44061 | 72911 | 118480 | 217172 |
| 2016 | 126224 | 24002 | 45600 | 74869 | 121018 | 225251 |

a [[https://fas.org/sgp/crs/misc/RS20811.pdf][report]] from the Congressional Research Service gives nice numbers
for 2012.  this probably comes from (the 2012-version of) [[https://www.census.gov/data/tables/time-series/demo/income-poverty/cps-hinc/hinc-06.html][hinc-06]],
from the Census Bureau.  sadly, hinc-06.xls seems to go back only a
few years.

hinc-06.xls: 2017 2016 2015 2014 2013
hinc-06_000.xls: 2012
new06_000.txt: 2003

[[https://usa.ipums.org/usa/][ipums.org]] is some sort of data service.  (it uses [[http://www.nber.org/data/current-population-survey-data.html][NBER data]].)  the
ipums data is unaggregated.  about 2MB for a file (1995).  and, of
course, many variables i don't understand.  plus, in nominal dollars.
but, the fact that it is unaggregated means that one can put in real
dollars *before* binning.  (though, when looking at a CDF, one can
convert each year's bin's into real dollars after the fact without
affecting things.)

[[http://www.pressure.to/works/hbai_in_r/][households below average income]] analysis in R.  for UK data, however.

[[https://www.kdnuggets.com/2014/06/data-visualization-census-data-with-r.html][data-visualization-census-data-with-r]].  old, broken links, etc.

[[https://www.r-bloggers.com/how-to-make-maps-with-census-data-in-r/][how-to-make-maps-with-census-data-in-r]] is newer.

[[http://users.stat.umn.edu/~almquist/software.html][Zach Almquist]] has 10-year census data.

[[https://www.bls.gov/cps/][BLS]] CPS page.  however, "All self-employed persons are excluded,
regardless of whether their businesses are incorporated."

the [[https://statisticalatlas.com/United-States/Household-Income][Statistical Atlas]] has nice graphics (though maybe not time
series).  from American Community (?) Survey.

[[https://www.cbpp.org/research/poverty-and-inequality/a-guide-to-statistics-on-historical-trends-in-income-inequality][a-guide-to-statistics-on-historical-trends-in-income-inequality]].

the [[https://www.cbo.gov/publication/51361][CBO]] has data, but mostly quintile-level.

[[https://cps.ipums.org/cps-action/downloads/extract_files/cps_00002.xml][IPUMS columns]]:
- YEAR
- [[https://cps.ipums.org/cps-action/variables/SERIAL][SERIAL]]: household serial number
- [[https://cps.ipums.org/cps-action/variables/HWTSUPP#codes_section][HWTSUPP]]: household weight, Supplement
- [[https://cps.ipums.org/cps-action/variables/CPSID#codes_section][CPSID]]: CPS household record
- [[https://cps.ipums.org/cps-action/variables/ASECFLAG][ASECFLAG]]: flag for ASEC
- [[https://cps.ipums.org/cps-action/variables/HHINCOME][HHINCOME]]: total household income
- [[https://cps.ipums.org/cps-action/variables/MONTH][MONTH]]: the calendar month of the CPS interview
- [[https://cps.ipums.org/cps-action/variables/PERNUM][PERNUM]]: person number in sample unit
- [[https://cps.ipums.org/cps-action/variables/CPSIDP][CPSIDP]]: CPSID, person record
- [[https://cps.ipums.org/cps-action/variables/WTSUPP#description_section][WTSUPP]]: supplement weight

to format one file:
#+BEGIN_SRC sh :results output
  ((zcat ipums/cps_00001.csv.gz | head -1 | sed 'sx"xxg' | sed s'x,x xg');
   (zcat ipums/cps_00001.csv.gz | tail -n+1 | sed s'x,x xg' | sort -n -k6)) |
      column -t
#+END_SRC

#+RESULTS:


#+BEGIN_SRC awk :shebang "#!/usr/bin/awk -f" :tangle realize
  BEGIN {
      FS = ",";
      OFS = ",";
  }

  FNR == 1 {
      fileno++;
      if (fileno == 2) {
          print $0 OFS "\"RHHINCOME1999\"";
      }
      next;
  }

  fileno == 1 {
      realities[$1] = $2;
  }

  fileno == 2 {
      if ($7 == "") {
          $7 = 0;                 # make later stage processing easier
      }
      print $0 OFS realities[$1]*$7;
  }
#+END_SRC

#+BEGIN_SRC sh :shebang "#!/usr/bin/env bash" :results none
./realize <(zcat ipums/cps_00004.csv.gz) <(zcat ipums/cps_00002.csv.gz)
#+END_SRC

i'll probably have to recode all this as an R script.  how to read a
gzipped file?  [[http://grokbase.com/t/r/r-help/016v155pth/r-read-data-in-from-gzipped-file][one set of thoughts]].
: x <- gzfile("./ipums/cps_00006.csv.gz", open="r")
: y <- read.csv(x, header=TRUE)
does the right thing.

getting a file from IPUMS, extract request like this:
#+BEGIN_QUOTE

EXTRACT REQUEST (HELP)

SAMPLES:56 (show) [samples have notes] Change
VARIABLES:12(show) Change
DATA FORMAT: .csv  Change
STRUCTURE: Rectangular (person)  Change
ESTIMATED SIZE:642.4 MB 
 
OPTIONS


Data quality flags are not available for any of the variables you've
selected.

Case selection is not available for any of the variables you've
selected.

Attach data from mother, father, spouse or household head as a new
variable (for example, education of mother).  Describe your extract
#+end_quote

** deflating

need to change from nominal to real dollars.  [[https://www.dallasfed.org/research/basics/nominal.cfm][Dallas Fed]] has some
explanation.

[[https://cps.ipums.org/cps/cpi99.shtml][IPUMS]] has a variable, [[https://cps.ipums.org/cps-action/variables/CPI99][CPI99]], that can be used to convert everything
to/from 1999 dollars.


** citing IPUMS

#+BEGIN_QUOTE
Publications and research reports based on the IPUMS-CPS database must
cite it appropriately. The citation should include the following:

Sarah Flood, Miriam King, Steven Ruggles, and J. Robert
Warren. Integrated Public Use Microdata Series, Current Population
Survey: Version 5.0 [dataset]. Minneapolis, MN: University of
Minnesota, 2017.  https://doi.org/10.18128/D030.V5.0

For policy briefs or articles in the popular press that use the
IPUMS-CPS database, we recommend that you cite the use of IPUMS-CPS
data as follows:

IPUMS-CPS, University of Minnesota, www.ipums.org
#+END_QUOTE

real file is: ./ipums/cps_00006.csv.gz
#+BEGIN_SRC R :session ss :var fname="./ipums/19712014.csv.gz"
  require(ggplot2)

  TOOHIGH = 10000000

  x <- gzfile(fname, open="r")
  dset <- read.csv(x, header=TRUE)
  # years are factors in our usage
  dset$YEAR <- as.factor(dset$YEAR)
  dset <- cbind(dset, HHINCOME1999=dset$HHINCOME*dset$CPI99)
  # sort
  dset <- dset[order(dset$YEAR, dset$HHINCOME1999),]
  # negative incomes?  describe
  nrow(dset[dset$HHINCOME1999<0,])
  summary(dset[dset$HHINCOME1999<0,"HHINCOME1999"])
  # now, get rid of negative incomes
  dset <- dset[dset$HHINCOME1999 >= 0,]
  # unrealistically (?) high incomes?  describe
  nrow(dset[dset$HHINCOME1999>TOOHIGH,])
  # (use HHINCOME, since we'd like to understand *reported* [recorded?]
  # value)
  summary(dset[dset$HHINCOME1999>TOOHIGH,c("YEAR","HHINCOME")])
  # now, get rid of all of those
  dset <- dset[dset$HHINCOME1999 <= TOOHIGH,]
  # cumulative sums of [H]WTSUPP (relies on being ordered)
  dset <- cbind(dset, CUMWTSUPP=0, CUMPCTWTSUPP=0, CUMHWTSUPP=0, CUMPCTHWTSUPP=0)
  # https://stackoverflow.com/a/32487458 on computing cumpct
  for (year in unique(dset$YEAR)) {
    s <- dset$YEAR == year
    cumwtsupp = sum(dset[s,]$WTSUPP)
    dset[s,]$CUMWTSUPP <- cumsum(dset[s,]$WTSUPP)
    dset[s,]$CUMPCTWTSUPP <- dset[s,]$CUMWTSUPP/cumwtsupp
    cumhwtsupp = sum(dset[s,]$HWTSUPP)
    dset[s,]$CUMHWTSUPP <- cumsum(dset[s,]$HWTSUPP)
    dset[s,]$CUMPCTHWTSUPP <- dset[s,]$CUMHWTSUPP/cumhwtsupp
  }


#+END_SRC

#+RESULTS:

