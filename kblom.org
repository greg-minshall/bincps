* [[https://twitter.com/kltblom][Kristian Blom]]: does recent (40+ years) change in US income distribution matter?
** misc configuration, etc., stuff                                 :noexport:
#+title: A small confirmation of some fairly well-known facts regarding the distribution of (household) incomes in the US
#+property: header-args :noweb yes
#+property: header-args:R :session ss
#+HTML_DOCTYPE: html5
#+OPTIONS: html-link-use-abs-url:nil html-postamble:auto
#+OPTIONS: html-preamble:t html-scripts:t html-style:t
#+OPTIONS: html5-fancy:t
#+OPTIONS: toc:nil
#+html_head_extra: <link href="lib/kbstyle.css" rel="stylesheet"/>
to change how the result is "wrapped", customize the variable
org-babel-inline-result-wrap.  the following means that the results of
inline calls are "raw".
#+bind: org-babel-inline-result-wrap "%s"

# our canonical file name...
#+name: ifile
#+BEGIN_SRC sh :cache yes
make binned
#+END_SRC

#+RESULTS[f43f76bafeb827b627f0a9cc98f177c4a94a9594]: ifile :cache yes
: ./ipums/cps_00006.1962-2017-binned.csv

# our years
#+name: years
#+BEGIN_SRC sh :cache yes :results verbatim
(make fyear; make lyear) | tr '\n' ',' | sed 's/,$//'
#+END_SRC

#+RESULTS[fea825c4ca5c095b74fa62be088b072ae715f74b]: years
: 1972,2016

** image suffix                                                    :noexport:

the following takes (what should be links ending in) strings that end
in ".IMG" and changes them to end in either ".pdf" or ".png",
depending on whether the current export is to latex or html.

this is definitely a sledge hammer approach.  there are other
techniques that seem promising, in particular using org-mode's
'#+BIND' keyword with org-export-filter-link-functions, but i was
unable to make that work.

this code runs as org-mode starts to export, and changes everything
under the sun.

#+name: get-image-names
#+BEGIN_SRC emacs-lisp :exports results :results none
  (defun get-image-names (backend)
    (let ((rment (if (equal backend 'html) ".png" ".pdf"))
          (case-fold-search nil))
      (while (re-search-forward "[.]IMG\\>" nil t)
        (replace-match rment t))))
  (add-hook 'org-export-before-parsing-hook 'get-image-names)
#+END_SRC


#+name: warning
#+BEGIN_SRC R :exports none :eval never
  ## WARNING:
  ##
  ## this file is generated from the emacs .org file "kblom.org" via
  ## "tangling".  any modifications to this file will be lost the next
  ## time the .org file is tangled.  this file is provided for the use
  ## of users who don't use emacs, or don't use org-mode.
  ## 
#+END_SRC
** javascript, css                                                 :noexport:

#+begin_src js :exports none :tangle lib/kbcode.js
  // tabbed info-box, from
  // https://developer.mozilla.org/en-US/docs/Learn/CSS/CSS_layout/Practical_positioning_examples

  var appendix = document.querySelector('.appendix');
  var tabs = document.querySelectorAll('.appendix li a');
  var panels = document.querySelectorAll('.appendix article');

  for(i = 0; i < tabs.length; i++) {
    var tab = tabs[i];
    setTabHandler(tab, i);
  }

  function deleteword(string, word) {
      // delete (the first occurrence of) WORD in STRING
      return string.replace(new RegExp(" *\\b" + word + "\\b *"), "")
  }

  function setTabHandler(tab, tabPos) {
      tab.onclick = function() {
          for(i = 0; i < tabs.length; i++) {
              tabs[i].className = deleteword(tabs[i].className, "active");
          }

          tab.className += ' active';

          for(i = 0; i < panels.length; i++) {
              panels[i].className = deleteword(panels[i].className, "active-panel");
          }

          panels[tabPos].className += ' active-panel';
      }
  }

  // okay, to get box height right...

  function tabbedloaded() {
      var height = Math.max(appendix.scrollHeight,
                            appendix.offsetHeight,
                            appendix.clientHeight)
      var width = Math.max(appendix.scrollWidth,
                            appendix.offsetWidth,
                            appendix.clientWidth)
      appendix.style.height = height + "px";
      appendix.style.width = width + "px";
  }

  // prime to wait for that
  appendix.addEventListener("load", tabbedloaded, false);
  tabbedloaded();                 // just in case
#+end_src

#+begin_src css :exports none :tangle lib/kbstyle.css
  /* https://stackoverflow.com/a/5887767 */
  table {
      table-layout:fixed;
      border-collapse:collapse;
      border-spacing:0;
  }
      td {
          padding:0.25em;
          border:1px solid black;
      }
#+end_src

[[https://developer.mozilla.org/en-US/docs/Learn/CSS/CSS_layout/Practical_positioning_examples][tabbed info-box]]

#+begin_src css :exports none :tangle lib/kbstyle.css
  .appendix {
    min-width: 960px;             /* to fit timeline */
  }

  .appendix ul {
    overflow: auto                /* as above, clear child's float */
  }

  /* the tabs */
  .appendix li {
    float: left;                  /* to get to stack left-to-right */
    list-style-type: none;        /* nothing in front of list items */
    width: 18%;                   /* there are 5, leave some extra */
  }
  .appendix li a {
    display: inline-block; /* like a box, but inline */
    text-decoration: none; /* to keep <a...> underline from appearing */
    width: 100%;           /* fill up space allowed by parent li */
    line-height: 4;   /* make the surrounding box a bit more generous */
    background-color: lightgrey;
    color: black;
    text-align: center;
    border: solid 1px;
    border-collapse: collapse;    /* so we don't get double*/
  }
  .appendix li a:focus, .appendix li a:hover {
    background-color: darkgrey;
    color: black;
  }
  .appendix li a.active {
    background-color: darkgrey;
    color: black;
  }

  /* the panels */
  .appendix .panels {
      position: relative;
  }
  .appendix article {
    position: absolute;
    top: 0;
    left: 0;
    visibility: hidden;
  }

  .appendix .active-panel {
    visibility: visible;
  }
#+end_src

** Introduction

in a [[https://twitter.com/kltblom/status/932394678241988609][Tweet]] from 19 November, 2017, Kristian Blom showed [[file:./DPCIA2AUQAEO0lv.jpg][a histogram]]
(1971-2015), from Financial Times (based on data from Pew Trust).
Blom asked, given that the past forty-plus years has seen so many
people do better, should we be worried about the growing inequality?

Blom showed the following [[file:images/DPCIA2AUQAEO0lv.IMG][graphic]], apparently taken from a video from
the Financial Times:
#+name: fig:ftdisplay
[[file:images/DPCIA2AUQAEO0lv-small.IMG]]

While Blom's question was asking for a qualitative answer (given that
the issues are really sociological), here I treat the question
quantitatively.

** Step one: reproduce the display

#+BEGIN_SRC R :tangle bin/create.repro :shebang "#!/usr/bin/env Rscript" :results none :eval no-export :exports none
  <<warning>>
  <<looking.bits>>
  <<stats.binned>>


  repro.chart <- function(dset,
                          bracket="HHBRACKET99",
                          supp="HWTSUPP",
                          years=c(1972, 2016),
                          binsize=5000, # FT used binsize 5000
                          tgt.in.key) {
    df <- data.frame()
    for (year in years) {
      yset <- dset[dset$YEAR==year,]
      newvals <- rebinvals.binned(min(yset[,bracket], na.rm=TRUE),
                                  max(yset[,bracket], na.rm=TRUE), binsize)
      newnobs <- rebin.binned(vals=yset[,bracket], nobs=yset[,supp],
                              newvals=newvals)
      if (tgt.in.key != "bracket/supp") {     # XXX
        newnobs <- newnobs*newvals        # total income in each bracket
      }
      dfnew <- data.frame(vals=newvals, nobs=newnobs)
      dfnew <- cbind(dfnew, year=as.factor(year), tobs=sum(dfnew$nobs))
      df <- rbind(df, dfnew)
    }

    base <- ggplot(df, aes(vals, nobs/tobs, colour=year)) + geom_step()

    print(base)                           # actually display

    ## by the way, the differences between the two curves should sum to
    ## (approx) zero
    sh <- df$year==min(years)
    sl <- df$year==max(years)
    sbzero <- sum(df[sh,"nobs"]/df[sh, "tobs"] - df[sl,"nobs"]/df[sl,"tobs"])
    if (abs(sbzero) > 0.001) {
      stop(sprintf("[approx] integral between curves s/b zero, is %g", sbzero))
    }
    return(base)
  }


  main <- function(args=NULL) {
    dset <- doparser(name="create.repro",
                     need=c("ifile",
                       "maxhh", "supp", "bracket",
                       "binsize",
                       "graphics", "gfile", "gopts",
                       "height", "width",
                       "years", "tgt.in.key"),
                     args)

    repro.chart(dset, bracket=attr(dset, "bracket"), supp=attr(dset, "supp"), years=attr(dset, "years"), tgt.in.key=attr(dset, "tgt.in.key"))
    endup(dset)
    return(0)
  }

  <<looking.endbits>>
#+END_SRC

To understand the effects of the change in the distribution of income,
it is useful to look at the (empirical) cumulative distribution
function ([[CDF]]) of the data for the years in question.  The following
graphic (which is presented in 1999 dollars, not the 2014 dollars of
the FT graph) using publicly available data from [[http://www.ipums.org][IPUMS-CS]], attempts to
reproduce the FT graphic Blom tweeted:
#+name: fig:repro
[[file:graphs/repro.IMG]]

This is not by any means exactly as the FT graphic.  However, if you
squint, it is roughly the same.

** Looking closer at the data

If you will grant me that the two graphs represent something like the
same distribution of household incomes, then we can now look at the
CDF of the dataset underlying the graphic:

#+BEGIN_SRC sh :tangle bin/create.ecdf :shebang "#!/bin/sh" :results none :eval never :exports none
  ./bin/create.ecdf.common $*
#+END_SRC

#+name: fig:ecdf
[[file:graphs/ecdf.IMG]]

In the above, we see that about 30% of US households have not done
much better in the years since 1972 (compared to how well they were
doing in 1972).  If we blow up the section for household incomes below
$50,000, we can see what happens around that area in more detail:

#+BEGIN_SRC sh :tangle bin/create.ecdf2 :shebang "#!/bin/sh" :results none :eval never :exports none
  ./bin/create.ecdf.common $* --maxhh 50000
#+END_SRC

#+name: fig:ecdf2
[[file:graphs/ecdf2.IMG]]

Here it becomes clearer that the lowest 30% of US households (ranked
by income) are doing worse in 2016 than they had been doing in 1972.

In 1972, 23% of US households made less than $20,000; by 2016, that
number has increased to 26%. ???

** Appendices
   :PROPERTIES:
   :HTML_CONTAINER_CLASS: furtherinfo
   :HTML_CONTAINER: section
   :END:

Pick a tab, any tab...

#+name: furtherinfoul
#+begin_export html
 <ul>
    <li><a href="#furtherinfoul" class="active">Cumulative Distribution Functions (CDFs)</a></li>
    <li><a href="#furtherinfoul">Methodological problems</a></li>
    <li><a href="#furtherinfoul">Most reported incomes</a></li>
    <li><a href="#furtherinfoul">Which groups get what percent of the national income?</a></li>
    <li><a href="#furtherinfoul">Where have all the flowers gone?</a></li>
    <li><a href="#furtherinfoul">Age issues</a></li>
    <li><a href="#furtherinfoul">Oddities</a></li>
  </ul>
#+end_export

*** 
   :PROPERTIES:
   :HTML_CONTAINER_CLASS: panels
   :HTML_CONTAINER: div
   :END:

*** TODO CDF

[[https://en.wikipedia.org/wiki/Probability_distribution][Probability distributions]] [fn::In this note, we are discussing
distributions over the real numbers, as opposed to, say, over some
geographic region] come in two basic forms: discrete and continuous.
In a discrete distribution, the probability of an event happening at a
given point is either zero, or a positive number greater than zero but
less than one; the probability will only be non-zero for an at-most
countable number of points, presumably (XXX continuum hypothesis); at
least *one* point has a positive probability; and the sum of the
probabilities of all the points at which the probability is positive
adds to one; i.e., *some* outcome occurs.

In a continuous distribution, in contrast, the probability of a given
outcome can be zero for *all* possible outcomes (points), but still,
the *integral* over all the possible outcomes will be one; i.e.,
*some* outcome still occurs.  In a continuous distribution, it can
still be the case that one or more of the points (again, presumably at
at most countably many points) may have a positive probability of
occurring, but the sum of the probabilities over all such points will
be less than one (otherwise, the distribution would, in fact, be a
discrete distribution).

When displaying a discrete probability distribution, it is common to
show a series of "stair steps", going up and down, up and down, with
each step centered on one (or more) of the points where the
distribution takes on a new value.

A common technique for continuous probability distributions
(especially for a "[[https://en.wikipedia.org/wiki/Frequency_distribution][frequency distribution]]", or a so-called "[[https://en.wikipedia.org/wiki/Empirical_distribution_function][empirical
distribution function]]", i.e., for cases where one is looking not at
the theoretical distribution, but rather the results of a number of
experiments) is to form a [[https://en.wikipedia.org/wiki/Histogram][histogram]], "binning" the possible values (so
that all the values from, e.g., 1-10 are counted in one bin, from
11-20 counted in another, etc.), and then assigning each bin a
"probability" based on the number of counts in that bin divided by the
total number of counts over all the bins.  This histogram can then be
displayed, looking something like the rose-colored bars in the
Financial Times [[fig:ftdisplay][display]] Blom posted.  For example, according to the FT
display, in 2015, a household income of $50,000 has about a four
percent chance of occurring.

The "[[https://en.wikipedia.org/wiki/Probability_mass_function][probability mass function]]" (PMF -- looking something like the
solid blue line floating "above" the bars in the FT [[fig:ftdisplay][display]]) is the
curve you would get if you let the "width" of the histogram bins "go
to" zero, i.e., get thinner and thinner, approaching a width of zero.
In the FT display, in 1971, a household income of about $10,000 had
about a four percent chance of occurring (as did a household income of
about $75,000 in that same year).

The plot of the histogram or the PMF is useful, showing, as it does,
places where the likelihood is (locally) higher than in other places.

As mentioned above, similar as to the fact there are two types of
*distributions* (discrete and continuous), there are two types of
distribution *functions*: theoretical and empirical.  The theoretical
distribution function of a distribution, as the name says, is what the
distribution would look like if it followed its theoretical
distribution.  On the other hand, the empirical distribution function
describes the results of a series of experiments, a long series of
"draws" from the distribution in question.  While the theory of
statistics, in particular [[https://en.wikipedia.org/wiki/Law_of_large_numbers][the law of large numbers]], guarantees that
over the long run, the empirical distribution of a series of draws
from a given distribution will approach the theoretical distribution,
it is also the case that, as [[https://en.wikiquote.org/wiki/John_Maynard_Keynes][Keynes said]], "But this long run is a
misleading guide to current affairs.  In the long run we are all
dead".  And, as it is often the case, looking at real world
"experiments", we aren't even certain of which theoretical
distribution, with which parameters, we are looking at, it is often
more valuable to look at empirical distributions.

In addition to the PMF (or, for continuous distributions, the
"[[https://en.wikipedia.org/wiki/Probability_density_function][probability density function]]", PDF), there's another way of looking
at, characterizing, a distribution, the "[[https://en.wikipedia.org/wiki/Cumulative_distribution_function][cumulative distribution
function]]" (CDF).  The CDF can never go down -- it can only go up (or
stay the same; we say the function is "[[https://en.wikipedia.org/wiki/Monotonic_function][monotonic]]", but not "strictly
monotonic").  It is always in the range between zero and one; it
starts at zero at negative infinity (say), and will have reached one
by the time it arrives at positive infinity (often before).  The value
of the cumulative distribution function at a given point is
probability that the a sample taken from the distribution will be less
than or equal to that point.  Oof -- what does that mean?

Take a look at the [[fig:ecdf2][closeup CDF]].  Notice that the 1972 and the 2016
lines cross at about 30% of the population (ranked by household
income), and that this corresponds to a household income of about
$30,000.  What this means is that in both 1972 and in 2016, about 30%
of the population had an income of less than, or equal to, $30,000.

(In a plot of the CDF, by the way, places where the likelihood is
locally higher than in other places is seen by a place where the slope
of the curve is very high, i.e, where the curve is steep.)

It takes a while to understand what one is looking at with a plot of a
CDF.  So, why bother?  I.e., what are the advantages of a CDF over a
PDF?  One big advantage is when trying to compare two distributions.
If one of the two PDFs isn't clearly below the other, i.e., if they
alternate being larger, it's sometimes easier to understand

In a case like the one we are looking [[fig:ecdf2][at]], we pointed out above that
the 1972 and 2016 CDFs crossed at about $30,000.  Thus, about the same
percentage of households in 2016 were making at least $30,000 as
in 1972.  But, look at $25,000.  In 1972, almost exactly (fortuitous
for expository purposes!) 25% of US households were making $25,000 or
less.  On the other hand, in 2016, 26%, 27%, or so of US households
were making $25,000 or less.  I.e., one or two percentage points of US
households in 2016 were making *less* than in 1972.

*** Methodological problems

**** TODO High income issues

#+BEGIN_SRC R :tangle bin/create.histo.hhbracket99 :shebang "#!/usr/bin/env Rscript" :results none :eval no-export :exports none
  <<warning>>
  <<looking.bits>>
  <<stats.binned>>

  histo.hhbracket99 <- function(dset, years) {
    dset$YEAR <- as.factor(dset$YEAR)
    base <- ggplot(dset[dset$YEAR %in% years,],
                   aes(HHBRACKET99, NRESP, colour=YEAR))
    base <- base + geom_col()
    print(base)
  }

  main <- function(args=NULL) {
      dset <- doparser(name="totalincome",
                       need=c("ifile",
                              "maxhh", "supp", "bracket",
                              "binsize",
                              "graphics", "gfile", "gopts",
                              "height", "width",
                              "years"),
                       args)
      histo.hhbracket99(dset, years=attr(dset, "years"))
      endup(dset)
      return(0)
  }

  <<looking.endbits>>

#+END_SRC

#+begin_src sh :tangle bin/create.histo.hhbracket99-both :shebang "#!/bin/sh" :results none :eval no-export :exports none
./bin/create.histo.hhbracket99 $*
#+end_src
#+begin_src sh :tangle bin/create.histo.hhbracket99-1972 :shebang "#!/bin/sh" :results none :eval no-export :exports none
./bin/create.histo.hhbracket99 $*
#+end_src

It's tricky trying to get reliable data at the high end of the income
scale (XXX Piketty, Saez, et al.)  In the following graphic, we show
number of data points per income group (i.e., *not* multiplied by the
weighting factor [[https://cps.ipums.org/cps-action/variables/HWTSUPP#codes_section][HWTSUPP]]).

file:graphs/histo.hhbracket99-both.IMG

Ignoring the hump around 200,000 dollars for the year 1972 (see the
section [[Oddities]]), it is clear that very few respondents have high
incomes.

While it's not apparent in this graphic, incomes below around 5,000,
6,000 dollars also have a small number of respondents.  If we just
look at 1972, we see this:

file:graphs/histo.hhbracket99-1972.IMG

**** TODO Low income issues

In addition to the problems of getting enough data points, low income
quality of life may be highly influenced by social "safety net"
(welfare) programs.  So, loss of N% over 40 years may not translate to
an N% lower quality of life, as the social safety net in the US has
changed dramatically over that period of time.  We are not experts in
this area, and so cannot make any certain pronouncements about how
these changes may either offset or reinforce the trend seen in
household income over this period of time (The Economist has [[https://www.economist.com/news/finance-and-economics/21739662-estimates-income-growth-vary-greatly-depending-methodology-average][something
to say]] on this point).

However, our charts tend to emphasize the *percentage* difference
between two groups, or between the same group over two different
years.  For middle- and high-income families, that may be very
reasonable, but for low-income families, a small *percentage*
difference in income may well be very significant in terms of, e.g.,
the ability to manage monthly expenses.

*** TODO Most reported incomes

The IPUMS data, the Census American Community Survey data, is based on
self-reporting.  While the Census Bureau will try to obtain valid
responses, respondents will likely, at the very least, estimate their
annual income, rather than give a precise answer.  What are the
incomes that people tend to report?  The following "rankboard" display
shows the answer for several years during the period of interest.  In
the display, each column shows the most popular household income
answers, from top (the most popular) to bottom (least popular among
these most popular answers).

file:graphs/mostreported.IMG



*** Who gets what percent of the national income?

A lot of money is paid out in income in the United States.  How is
this money, the total household income (the total of the amounts of
money reported as household income, henceforth, "total income"
[fn::I.e., the sum of the product of each of the amount reported as a
household income times the number of households reporting that
amount]) distributed among the various income groups?  Here is a CDF
which shows what percent of the total national income is paid out to
which income groups in the US.

First we show, in a table form, how much of the total income was
earned by different percentiles of households in the United States,
for the years 1972 and 2016.  We see that, for example, in 1972,
call_lucr(keys=0.5,tgtinkey="%totalincome:%population",years=1972,docomprss="FALSE")
of the total US income was earned by the less wealthy half of US
households (by income), whereas, in 2016, the less wealthy half of US
households earned only
call_lucr(keys=0.1,tgtinkey="%totalincome:%population",years=2016,docomprss="FALSE")
of the total US income.


#+name: tincdistrbytile
#+begin_src R :results table :colnames yes :exports results
  <<create.tiles>>
  df <- tiles.main(c("--maxhh", "2e14", "--tgt.in.key", "%totalincome:%population", "--tiles", ".1,.25,.5,.75,.9,1", "--diff"))
  round(df,4)
#+end_src

#+RESULTS: tincdistrbytile
| Year |    0.1 |   0.25 |    0.5 |   0.75 |    0.9 |      1 |
|------+--------+--------+--------+--------+--------+--------|
| 1972 | 0.0169 | 0.0598 | 0.1786 | 0.2733 | 0.2213 | 0.2501 |
| 2016 | 0.0089 |  0.044 | 0.1358 | 0.2468 |   0.24 | 0.3245 |

Here we notice that the top ten percent of income earners (from 0.9
to 1) has increased its share of the total US household income from 25
percent in 1972 to almost 33 percent in 2016 while, at the same time,
the bottom ten percent (from 0 to 0.1) has seen *its* share of the
total income decrease by about one half.  This is to the point I
understood to be Blom's question, i.e, even *if* the *share* of the
bottom income bracket fell, do we really care, assuming (which turns
out to be an untrue assumption) that the *actual* household income of
that bottom income bracket in fact increased.

Now we look at the same data, however looking at the total income
earned by each household income bracket.

#+name: tincdistrbybracket
#+BEGIN_SRC sh :tangle bin/create.tincdistrbybracket :shebang "#!/bin/sh" :results none :exports none :eval never :var years=years
  ./bin/create.ecdf.common $* --years ${years} --tgt.in.key %totalincome:bracket --maxhh 220000
#+END_SRC

#+caption: How the total US income in 1972 and in 2016 was distributed across US households (sorted by income)
[[file:graphs/tincdistrbybracket.IMG]]

*** Where have all the flowers gone

#+name: comprss
#+BEGIN_SRC R :exports none :eval no-export
  ## https://stackoverflow.com/a/28160474
  comprss <- function(number, words=FALSE, precision=2) {
      "output a (presumably) large NUMBER with trailing units, at a specified PRECISION"
      if (words) {
          units <- c("", "thousand", "million", "billion", "trillion")
      } else {
          units <- c("","K","M","B","T")
      }
      div <- findInterval(abs(as.numeric(gsub("\\,", "", number))),
                          c(1, 1e3, 1e6, 1e9, 1e12))

      div[div==0] <- 1
      paste(round(as.numeric(gsub("\\,","",number))/10^(3*(div-1)),
                  precision),
            units[div])
  }
#+END_SRC

#+name: doprint
#+begin_src R :exports none :eval no-export
  <<comprss>>
  doprint <- function(vals, words=words, docomprss, format, diff, round) {
      if (diff && (length(vals)>1)) {
          vals <- diff(vals)
      }
      if (round) {
          vals <- round(vals, round)
      }
      if (docomprss) {
          vals <- comprss(vals, words=words)
      } else {
          if (format == "") {
              format <- NULL
          }
          ## Sys.localeconv()["thousands_sep"] == "" for some reason, so...
          vals <- prettyNum(vals, big.mark=" ", format=format)
      }
      sprintf("%s", vals)
  }
#+end_src

#+RESULTS: doprint

#+name: lucr
#+BEGIN_SRC R :exports none :var tgtinkey="totalincome:%population" :var keys="1" :var years="1972,2016" :var ifile=ifile :var words="FALSE" :var docomprss="TRUE" :var format="" :var diff=1 :var round=3
  <<lookup>>
  <<doprint>>

  df <- lookup.main(c("--ifile", ifile, "--maxhh", "2e14", "--years", years, "--tgt.in.key", tgtinkey, "--keys", keys))
  doprint(df[,3], words=words, docomprss=docomprss, format=format, diff=diff, round=round)
#+END_SRC

#+RESULTS: lucr
: 10.17 T

The total household income of the United States (the total of the
amounts of money reported as household income, henceforth, "total
income" [fn::I.e., the sum of the product of each of the amount
reported as a household income times the number of households
reporting that amount]) increased from
call_lucr(keys=1,years=1972,words="TRUE",docomprss="TRUE") dollars in
1972 to call_lucr(keys=1,years=2016,words="TRUE",docomprss="TRUE") in
2016, an increase of call_lucr(keys=1,years="1972,2016",words="TRUE")
dollars.  Who got all this money?  How was it shared among the
population?

The following graphic shows how the extra money earned in 2016 (over
that earned in 1972) has been shared among the income deciles in the US.

#+BEGIN_SRC sh :tangle bin/create.tidistr :shebang "#!/bin/sh" :results none :eval never :exports none
  ./bin/create.ecdf.common $* --years 1972,2016 --tgt.in.key %totalincome:%population --delta --maxhh 2e14
#+END_SRC

#+caption: How the increase in incomes of US households in 2016 over 1972 was distributed across deciles of the US popuation (sorted by income)
[[file:graphs/tidistr.IMG]]

While the above graphic may look to be the graph of a cumulative
distribution, it is not (it looks like a CDF because, well, "the rich
get richer").  It is, rather, the actual distribution, showing how
much of the excess income between 1972 and 2016 is earned by each of
the ten decile income groups in the US.  If the extra income had been
shared equally across the ten income groups, the ten bars would all
have the same height.  Instead, we see that the highest decile (the
highest ten percent of earners) has captured more than 35 percent of
the increased income, while the lowest ten percent has captured
effectively nothing of the increase in income over the previous 40+
years.

Note that this chart shows that within *all* (other than maybe the
lowest income decile) income deciles, there was more total income
received by that group in 2016 than in 1972.  This might seem to
contradict the main point of this note, i.e., that a large number of
lower income households in fact were in fact worse off in 2016 than
they were in 1972.  (And, of course, our methodology doesn't let us
look at, e.g., the Smith family from 1972 Main Street, and see how
their fortunes changed over the span of years.)  But, the reason for
the increase in *total* income being positive, whereas the income per
family is *negative* is that, in 2016 there were more families than in
1972, as the US population grew about 50 percent over that time
period.  So, the total income for a given group may have grown,
without the income per family growing.  [fn:1:The bars in the above
graph are *still* reflective of the overall inequality, as the number
of households in each income decile are the same.]

*** TODO Age issues

It would be nice to know the age of the head(s) of household for
reporting, as it is [[http://www.tsowell.com/basicecon_5.html][said]] to be the case that many "households" start
off in low income brackets, and make their way up.  If it were the
case that, e.g., the lowest income brackets were all made up of 20
year olds, but that by 30 years of age, almost all households have
moved into brackets which *have* seen their incomes increase over the
past forty years, then we would think of a different set of social
issues than if, rather, certain classes within our society find
themselves overly represented in the bottom ten percent without any
movement as the household members age.

*** Oddities

The IPUMS data set starts in 1962.  But, from 1962 through 1967, it
contains no actual reported incomes.

From 1968 through 1975, the reported incomes appear to have been
capped at 50,000 dollars.  In fact, incomes greater than 50,000
dollars [[https://cps.ipums.org/cps/inctaxcodes.shtml][were reported by the Census Bureau as 50,000 dollars]].  For
example, one sees a hump at the 1999 equivalent of 50,000 1972 dollars
(a bit over 200,000 1999 dollars) in the figure in [[High income issues]].

From 1976, onwards, we see no evidence of such a cap on reported
income.
** Acknowledgements

First, I would like to acknowledge the great database that [[http://www.ipums.org][IPUMS-CS]]
has made available:
#+BEGIN_QUOTE
Sarah Flood, Miriam King, Steven Ruggles, and J. Robert
Warren. Integrated Public Use Microdata Series, Current Population
Survey: Version 5.0 [dataset]. Minneapolis, MN: University of
Minnesota, 2017.  https://doi.org/10.18128/D030.V5.0
#+END_QUOTE

Second, my analysis has been done in the R statistical programming
environment, and my thanks to the team that develops R:
#+BEGIN_QUOTE
R Core Team (2017). R: A language and environment for statistical
computing. R Foundation for Statistical Computing, Vienna, Austria.
URL https://www.R-project.org/.
#+END_QUOTE

Third, my approach to anything statistical is indebted to [[https://en.wikipedia.org/wiki/John_Tukey][John Tukey]]'s
[[https://en.wikipedia.org/wiki/Exploratory_data_analysis][/Exploratory Data Analysis/]], a book I highly recommend.

#+html: <script src="lib/kbcode.js"></script>

** explorations, etc. :noexport:
*** where to find data?

**** [[https://www2.census.gov/programs-surveys/cps/tables/time-series/historical-income-households/h01ar.xls][Census Bureau]] has something that breaks down by each fifth and top 5%.

#+BEGIN_SRC R :var tseries=tseries :eval no-export :exports none
colnames(tseries) <- c("year", "number", "lowest", "second", "third", "fourth", "llimittop5")
#+END_SRC

#+RESULTS:
| year       |
| number     |
| lowest     |
| second     |
| third      |
| fourth     |
| llimittop5 |

however, the FT graph Blom showed has about fifty buckets, whereas the
Census data seems to have have six.

#+name: tseries
#+BEGIN_SRC sh :eval no-export :exports none
  xls2csv census/h01ar.xls |
      awk '/2016 Dollars/ { ok = 1; next} \
          /^"[12]/ {
                   if (ok) { 
                      gsub(/ *\([0-9][0-9]\) */, ""); 
                      gsub(/"/, ""); 
                      print;
                    }}' 2>&1 |
      tac
#+END_SRC

#+RESULTS: tseries
| 1967 |  60813 | 18856 | 36768 | 52186 |  74417 | 119419 |
| 1968 |  62214 | 20098 | 38103 | 54614 |  76737 | 120053 |
| 1969 |  63401 | 20699 | 39718 | 57441 |  80478 | 126218 |
| 1970 |  64778 | 20350 | 38985 | 56703 |  80899 | 127880 |
| 1971 |  66676 | 20088 | 38294 | 56353 |  80353 | 127602 |
| 1972 |  68251 | 20786 | 40033 | 59167 |  84686 | 136292 |
| 1973 |  69859 | 21238 | 40839 | 60425 |  87000 | 139832 |
| 1974 |  71163 | 21340 | 39585 | 58493 |  84892 | 134366 |
| 1975 |  72867 | 20288 | 38076 | 57536 |  82611 | 130365 |
| 1976 |  74142 | 20738 | 38636 | 58856 |  84678 | 134287 |
| 1977 |  76030 | 20694 | 38977 | 59411 |  86616 | 137142 |
| 1978 |  77330 | 21338 | 40346 | 61046 |  88785 | 142036 |
| 1979 |  80776 | 21594 | 40103 | 61700 |  89461 | 144557 |
| 1980 |  82368 | 20745 | 38905 | 59645 |  87332 | 140543 |
| 1981 |  83527 | 20340 | 38023 | 58809 |  86946 | 139925 |
| 1982 |  83918 | 20080 | 38191 | 58352 |  87015 | 143636 |
| 1983 |  85290 | 20516 | 38149 | 58550 |  88485 | 145579 |
| 1984 |  86789 | 20909 | 39134 | 60292 |  91077 | 150768 |
| 1985 |  88458 | 21154 | 39801 | 61657 |  92731 | 153220 |
| 1986 |  89479 | 21430 | 40990 | 63616 |  96164 | 161255 |
| 1987 |  91124 | 21835 | 41447 | 64697 |  97780 | 163619 |
| 1988 |  92830 | 22210 | 41953 | 65380 |  98722 | 167109 |
| 1989 |  93347 | 22614 | 43000 | 66089 | 100414 | 171533 |
| 1990 |  94312 | 22271 | 42159 | 64498 |  98359 | 168813 |
| 1991 |  95669 | 21646 | 41260 | 63729 |  97578 | 165727 |
| 1992 |  96426 | 21136 | 40494 | 63575 |  97304 | 166101 |
| 1993 |  97107 | 21217 | 40380 | 63473 |  98663 | 171210 |
| 1994 |  98990 | 21518 | 40389 | 64269 | 100717 | 176013 |
| 1995 |  99627 | 22536 | 42121 | 65734 | 101921 | 176848 |
| 1996 | 101018 | 22513 | 42318 | 67084 | 103684 | 182230 |
| 1997 | 102528 | 22979 | 43571 | 68640 | 106690 | 188834 |
| 1998 | 103874 | 23727 | 44768 | 71163 | 110418 | 194628 |
| 1999 | 106434 | 24702 | 46014 | 72630 | 114216 | 204698 |
| 2000 | 108209 | 24985 | 46009 | 72742 | 114000 | 202470 |
| 2001 | 109297 | 24361 | 45162 | 71849 | 113195 | 204021 |
| 2002 | 111278 | 23911 | 44545 | 70950 | 112127 | 200192 |
| 2003 | 112000 | 23468 | 44369 | 71059 | 113358 | 201120 |
| 2004 | 113343 | 23489 | 44059 | 70177 | 111818 | 199682 |
| 2005 | 114384 | 23570 | 44244 | 70864 | 112705 | 204014 |
| 2006 | 116011 | 23850 | 44967 | 71425 | 115508 | 207146 |
| 2007 | 116783 | 23489 | 45262 | 71770 | 115758 | 204892 |
| 2008 | 117181 | 23089 | 43476 | 69924 | 111744 | 200658 |
| 2009 | 117538 | 22880 | 43124 | 69134 | 111865 | 201359 |
| 2010 | 119927 | 22017 | 41832 | 67702 | 110116 | 198686 |
| 2011 | 121084 | 21617 | 41096 | 66609 | 108375 | 198438 |
| 2012 | 122459 | 21533 | 41568 | 67511 | 108818 | 199827 |
| 2013 | 122952 | 21535 | 41408 | 67492 | 109129 | 201957 |
| 2013 | 123931 | 21638 | 42282 | 69242 | 113582 | 211362 |
| 2014 | 124587 | 21728 | 41754 | 69153 | 113811 | 209419 |
| 2015 | 125819 | 23088 | 44061 | 72911 | 118480 | 217172 |
| 2016 | 126224 | 24002 | 45600 | 74869 | 121018 | 225251 |

**** Congressional Research Service

a [[https://fas.org/sgp/crs/misc/RS20811.pdf][report]] from the Congressional Research Service gives nice numbers
for 2012.  this probably comes from (the 2012-version of) [[https://www.census.gov/data/tables/time-series/demo/income-poverty/cps-hinc/hinc-06.html][hinc-06]],
from the Census Bureau.  sadly, hinc-06.xls seems to go back only a
few years.

hinc-06.xls: 2017 2016 2015 2014 2013
hinc-06_000.xls: 2012
new06_000.txt: 2003

([[https://www.census.gov/popclock/][US, world population clock]])

**** IPUMS

[[https://usa.ipums.org/usa/][ipums.org]] is a data service which uses [[http://www.nber.org/data/current-population-survey-data.html][NBER data]].  the ipums data is
unaggregated.  about 2MB for a file (1995).  and, of course, many
variables i don't understand.  plus, in nominal dollars.  but, the
fact that it is unaggregated means that one can put in real dollars
*before* binning.  (though, when looking at a CDF, one can convert
each year's bin's into real dollars after the fact without affecting
things.)

**** misc. other

[[http://www.pressure.to/works/hbai_in_r/][households below average income]] analysis in R.  for UK data, however.

[[https://www.kdnuggets.com/2014/06/data-visualization-census-data-with-r.html][data-visualization-census-data-with-r]].  old, broken links, etc.

[[https://www.r-bloggers.com/how-to-make-maps-with-census-data-in-r/][how-to-make-maps-with-census-data-in-r]] is newer.

[[http://users.stat.umn.edu/~almquist/software.html][Zach Almquist]] has 10-year census data;  [[https://www.jstatsoft.org/article/view/v037i06][paper]].

[[https://www.bls.gov/cps/][BLS]] CPS page.  however, "All self-employed persons are excluded,
regardless of whether their businesses are incorporated."

the [[https://statisticalatlas.com/United-States/Household-Income][Statistical Atlas]] has nice graphics (though maybe not time
series).  from American Community (?) Survey.

the [[https://www.cbo.gov/publication/51361][CBO]] has data (under "Data and Supplemental Information"), but
mostly quintile-level.

a very nice [[https://www.cbpp.org/][Center on Budget and Policy Priorities]] paper, [[https://www.cbpp.org/research/poverty-and-inequality/a-guide-to-statistics-on-historical-trends-in-income-inequality]["A Guide to
Statistics on Historical Trends in Income Inequality"]], points at "most
recent" [[http://eml.berkeley.edu/~saez/TabFig2015prel.xls][Piketty/Saez estimates]] (and, [[https://eml.berkeley.edu/~saez/saez-UStopincomes-2015.pdf][their paper]]).  the [[https://www.cbo.gov/publication/53597][newest (2018)
version]].

[[http://www.gapminder.org/data/][gapminder]] is another source of data in the world (not just US).

*** looking at the ipums data

the ipums data seems the easiest to use.

[[https://cps.ipums.org/cps-action/downloads/extract_files/cps_00002.xml][IPUMS columns]]:
- YEAR
- [[https://cps.ipums.org/cps-action/variables/SERIAL][SERIAL]]: household serial number
- [[https://cps.ipums.org/cps-action/variables/HWTSUPP#codes_section][HWTSUPP]]: household weight, Supplement
- [[https://cps.ipums.org/cps-action/variables/CPSID#codes_section][CPSID]]: CPS household record
- [[https://cps.ipums.org/cps-action/variables/ASECFLAG][ASECFLAG]]: flag for ASEC
- [[https://cps.ipums.org/cps-action/variables/HHINCOME][HHINCOME]]: total household income
- [[https://cps.ipums.org/cps-action/variables/MONTH][MONTH]]: the calendar month of the CPS interview
- [[https://cps.ipums.org/cps-action/variables/PERNUM][PERNUM]]: person number in sample unit
- [[https://cps.ipums.org/cps-action/variables/CPSIDP][CPSIDP]]: CPSID, person record
- [[https://cps.ipums.org/cps-action/variables/WTSUPP#description_section][WTSUPP]]: supplement weight

to format one file:
#+BEGIN_SRC sh :results output :eval no-export :exports none
  ((zcat ipums/cps_00001.csv.gz | head -1 | sed 'sx"xxg' | sed s'x,x xg');
   (zcat ipums/cps_00001.csv.gz | tail -n+1 | sed s'x,x xg' | sort -n -k6)) |
      column -t
#+END_SRC

#+RESULTS:

this would have been "tangled" (saved) as "realize".
#+BEGIN_SRC awk :shebang "#!/usr/bin/awk -f" :eval no-export :exports none
  BEGIN {
      FS = ",";
      OFS = ",";
  }

  FNR == 1 {
      fileno++;
      if (fileno == 2) {
          print $0 OFS "\"RHHINCOME1999\"";
      }
      next;
  }

  fileno == 1 {
      realities[$1] = $2;
  }

  fileno == 2 {
      if ($7 == "") {
          $7 = 0;                 # make later stage processing easier
      }
      print $0 OFS realities[$1]*$7;
  }
#+END_SRC

#+BEGIN_SRC sh :shebang "#!/usr/bin/env bash" :results none :eval no-export :exports none
./realize <(zcat ipums/cps_00004.csv.gz) <(zcat ipums/cps_00002.csv.gz)
#+END_SRC

i'll probably have to recode all this as an R script.  how to read a
gzipped file?  [[http://grokbase.com/t/r/r-help/016v155pth/r-read-data-in-from-gzipped-file][one set of thoughts]].
: x <- gzfile("./ipums/cps_00006.csv.gz", open="r")
: y <- read.csv(x, header=TRUE)
does the right thing.  in fact, it turns out that read.csv() will
detect a .gz file and do the right thing.

getting a file from IPUMS, extract request like this:
#+BEGIN_QUOTE

EXTRACT REQUEST (HELP)

SAMPLES:56 (show) [samples have notes] Change
VARIABLES:12(show) Change
DATA FORMAT: .csv  Change
STRUCTURE: Rectangular (person)  Change
ESTIMATED SIZE:642.4 MB 
 
OPTIONS

Data quality flags are not available for any of the variables you've
selected.

Case selection is not available for any of the variables you've
selected.

Attach data from mother, father, spouse or household head as a new
variable (for example, education of mother).  Describe your extract
#+END_QUOTE
(in fact, the data set, from 1962--2017, was 82MB compressed, 520MB
uncompressed.)

**** topcodes

ipums [[https://cps.ipums.org/cps-action/variables/HHINCOME#codes_section][HHINCOME]] uses "[[https://cps.ipums.org/cps/inctaxcodes.shtml][topcodes]]" (and bottom codes) to encode
exceptions.  the popular "99999999" means "Not in universe" (NIU) --
something about the respondent

#+name: foundtopcodes
#+BEGIN_SRC sh :eval no-export :exports none
  zcat ipums/cps_00006.csv.gz | \
      awk 'BEGIN{FS=","} {print $7}' | \
      grep '\<9999' | \
      words -f | \
      sort -n
#+END_SRC

#+RESULTS: topcodes
| -9999997 |  129 |
|    -9999 |  781 |
|     9999 |  194 |
|    99990 |   12 |
|    99991 |    4 |
|    99992 |    8 |
|    99994 |    9 |
|    99995 |    8 |
|    99996 |   16 |
|    99997 |   14 |
|    99998 |   12 |
|    99999 |  525 |
| 99999999 | 2704 |

between the two web sites above, and the topcodes list above, maybe
these are the topcodes
|----------+-------------|
| 99999999 | N.I.U.      |
| -9999997 | Bottom Code |

#+name: topcodealpha
#+begin_src R :exports none :results none :eval none
  tcodeprep <- function() {
      x <- t(matrix(list(99999999, "N.I.U.",                     # (Not in Universe)
                         -9999997, "< -$9999"),
                    nrow=2))
      y <- as.data.frame(x)
      colnames(y) <- c("value", "nomen")
      y$value <- unlist(y$value)
      y$nomen <- unlist(y$nomen)
      return(y)
  }

  topcodes <- tcodeprep()                 # make available globally

  topcodealpha <- function(vals) {
      "convert top (or bottom) codes in vals to some
  identifying character string"
      z <- match(vals, topcodes$value)
      vals[!is.na(z)] <- topcodes[z[!is.na(z)],]$nomen
      vals
  }
#+end_src

*** deflating

need to change from nominal to real dollars.  [[https://www.dallasfed.org/research/basics/nominal.cfm][Dallas Fed]] has some
explanation.

on the other hand, conveniently, [[https://cps.ipums.org/cps/cpi99.shtml][IPUMS]] has a variable, [[https://cps.ipums.org/cps-action/variables/CPI99][CPI99]], that can
be used to convert everything to/from 1999 dollars.

*** citing IPUMS

#+BEGIN_QUOTE
Publications and research reports based on the IPUMS-CPS database must
cite it appropriately. The citation should include the following:

Sarah Flood, Miriam King, Steven Ruggles, and J. Robert
Warren. Integrated Public Use Microdata Series, Current Population
Survey: Version 5.0 [dataset]. Minneapolis, MN: University of
Minnesota, 2017.  https://doi.org/10.18128/D030.V5.0

For policy briefs or articles in the popular press that use the
IPUMS-CPS database, we recommend that you cite the use of IPUMS-CPS
data as follows:

IPUMS-CPS, University of Minnesota, www.ipums.org
#+END_QUOTE

*** most occurring incomes

question:
#+BEGIN_EXAMPLE
length(unique(dset$HHINCOME1999))
[1] 55297
> length(dset$HHINCOME1999)
[1] 345582
#+END_EXAMPLE
so, what are the most occurring incomes?

#+BEGIN_EXAMPLE
> x <- dset$HHINCOME
> z <- tabulate(x)
> zz <- sort.int(z, index.return=TRUE, decreasing=TRUE)
> zz$ix[1:30]
 [1]  50000  10000  12000  30000  15000  40000  20000  25000  60000  11000
[11]   9000   8000  35000   6000  45000  13000  18000   7000  14000   5000
[21]  24000  70000  55000  75000  17000  80000  36000  16000 100000  32000
> zz$ix[1:300]
  [1]  50000  10000  12000  30000  15000  40000  20000  25000  60000  11000
 [11]   9000   8000  35000   6000  45000  13000  18000   7000  14000   5000
 [21]  24000  70000  55000  75000  17000  80000  36000  16000 100000  32000
 [31]   7500  28000  65000  22000  19000  42000  23000  90000  38000  48000
 [41]  10500  27000   6500  12500  34000  21000   4000  62000  85000   3000
 [51]  26000  52000  58000   9500   8500  33000   7800  47000  37000   8400
 [61]   4800  31000 120000 110000   9600  10200  10400  11500  14500  29000
 [71]   7200  49000  10100  44000  39000  72000   5500  46000  95000  43000
 [81]  54000  57000  10800  15600  78000  13200  11200  41000  56000  63000
 [91]  53000 150000   3600   2000  51000   5200   9200 130000  10700   4500
[101]  73000  66000   9100  68000  59000   9800  88000  76000  77000 105000
[111]  11300  61000   6600   8200  64000  98000  10300  13500   6200  12300
[121]  14400  12200  69000  97000   2400  12100  74000   1500  11700  84000
[131]   9300  17500  81000  16500  94000   9700  92000  11800  71000  83000
[141] 115000  15500  67000  82000  11100  18200  86000   8700 140000  15400
[151]  12600  14700   6800  14200   8300   8800  12400   8100   1200  12700
[161]   7400  79000  96000   8600  15200   8900 125000  10600  11600  12800
[171]   1800   3500   6400   7900   8520  18500  14300  20800  89000   5600
[181] 160000  11400  91000  19200  10900   4200  17100  87000 102000  14100
[191]  99000   9400  14800  15100  13300   7600   7100  13259  13800 103000
[201] 108000   6900  15300  16100  93000 113000   5700   6300  16300   5800
[211]   6700   7700 106000   2600   5100   9659   3900   7300  17200   2500
[221]  13100  16400  19500 135000   4900  16800   1000  13900   8652  25200
[231] 112000  17400  17600 118000  13400  26500   3200  13700  14600  16600
[241]  31200  20400 128000   2700  20500      1  15659   4680   9900  33600
[251] 104000  18100  13600 107000  14900  15800  11900 109000 145000   6100
[261]  15900  21600  26800 114000   5400  12900  21400   3300   4300  22800
[271] 117000 155000   5900  18900  20600  22200 170000  18600  22500   4700
[281]  21200 101000  19400  16700   3400  18800  20100  20200   4600  14459
[291] 116000 165000   8640  16200  25500  30200  31500  34500 111000    600
> zz$x[1:30]
 [1] 1821 1553 1270 1193 1176 1163 1070 1026  913  854  827  826  825  767  761
[16]  758  746  745  717  694  668  598  595  593  576  555  540  538  523  508
#+END_EXAMPLE

*** how to deal with exception reports?

one issue the light of which i've yet to see: should the "exception"
reports (of incomes less than less than MIN99, greater than or equal
to MAX99) be inserted as comments in the output .csv file, or output
to a separate file (rfile, in bincps), or printed on the console?

the argument for including such reports in the .csv file is that, in
this case, the .csv file becomes more self-describing.  (there's a bit
of self-description in the file name, and more could be put there,
though after a while that becomes very awkward.)  self-describing data
sets are a "GOOD THING" (and, even semi-self-describing data sets are,
at least, a "good thing").

the argument against including such reports in the .csv file is that
then a pure "read.csv(ifile)" won't work, as read.csv assumes one
doesn't use comments in .csv files (defaults to comment.char="").
while one can document (even in a comment in the .csv file itself!)
that the .csv file contains comments and that in, e.g., R, one needs
to call read.csv(..., comment.char="#"); however, a certain percentage
of potential users will get lost before finding that message and will
give up.  likely those same users -- plus, probably, a much broader
class of users -- won't think of looking inside the .csv file, so
won't see the comments describing the file, so won't be helped by
the so-called "self-description".

i think in an ideal world, i'd provide a command line switch that
would determine how to deal with these files.

*** performance tuning bincps

the original version of this code processed each (non-trivial) year in
about 5 minutes on my system.  this turned out to be due to my habit,
motivated by trying to save main store usage, of not creating
subsetted copies of the massive dataset 'dset', but rather just using
"filters".  code such as:
:       sna <- sy & snabit
and then accessing 'dset' via the filter
: if (nrow(yset[sna,]) != 0)

so, i modified the code to create a new dataset, 'yset', for each
year, then use filters to access inside that dataset while processing
that year's data.  this got the time to process a year's worth of data
to fall to 5-20 seconds.

i became curious to know how these numbers related to the number of
observations in each year.  here we use awk(1) to count how many
observations are used in each year.

#+name: yearpeople
#+BEGIN_SRC sh :cache yes :eval no-export :exports none
  zcat ipums/cps_00006.csv.gz |
      awk 'BEGIN { FS="," } /^[12]/ { print $1}' |
      words -f                    # words returns each word seen, along
                                  # with the number of times that word
                                  # was seen (the "-f", "frequency",
                                  # flag)
#+END_SRC

#+RESULTS[ea0344ff23fa76aad213c3cc0bd2cc671e5f2113]: yearpeople
| 1962 |  71741 |
| 1963 |  55882 |
| 1964 |  54543 |
| 1965 |  54502 |
| 1966 | 110055 |
| 1967 |  68676 |
| 1968 | 150913 |
| 1969 | 151848 |
| 1970 | 145023 |
| 1971 | 146822 |
| 1972 | 140432 |
| 1973 | 136221 |
| 1974 | 133282 |
| 1975 | 130124 |
| 1976 | 135351 |
| 1977 | 160799 |
| 1978 | 155706 |
| 1979 | 154593 |
| 1980 | 181488 |
| 1981 | 181358 |
| 1982 | 162703 |
| 1983 | 162635 |
| 1984 | 161167 |
| 1985 | 161362 |
| 1986 | 157661 |
| 1987 | 155468 |
| 1988 | 155980 |
| 1989 | 144687 |
| 1990 | 158079 |
| 1991 | 158477 |
| 1992 | 155796 |
| 1993 | 155197 |
| 1994 | 150943 |
| 1995 | 149642 |
| 1996 | 130476 |
| 1997 | 131854 |
| 1998 | 131617 |
| 1999 | 132324 |
| 2000 | 133710 |
| 2001 | 218269 |
| 2002 | 217219 |
| 2003 | 216424 |
| 2004 | 213241 |
| 2005 | 210648 |
| 2006 | 208562 |
| 2007 | 206639 |
| 2008 | 206404 |
| 2009 | 207921 |
| 2010 | 209802 |
| 2011 | 204983 |
| 2012 | 201398 |
| 2013 | 202634 |
| 2014 | 199556 |
| 2015 | 199024 |
| 2016 | 185487 |
| 2017 | 185914 |

  

this is from a run
: bincps1(ifile=ifile, dset, ofile=ofile, rfile=rfile, ofsep=ofsep, fyear=fyear, lyear=lyear, min1999=min1999, max1999=max1999);

#+name: yeartimes
| 1962 | Thu Nov 30 17:46:18 2017 |
| 1963 | Thu Nov 30 17:46:18 2017 |
| 1964 | Thu Nov 30 17:46:19 2017 |
| 1965 | Thu Nov 30 17:46:19 2017 |
| 1966 | Thu Nov 30 17:46:19 2017 |
| 1967 | Thu Nov 30 17:46:20 2017 |
| 1968 | Thu Nov 30 17:46:21 2017 |
| 1969 | Thu Nov 30 17:46:26 2017 |
| 1970 | Thu Nov 30 17:46:31 2017 |
| 1971 | Thu Nov 30 17:46:36 2017 |
| 1972 | Thu Nov 30 17:46:41 2017 |
| 1973 | Thu Nov 30 17:46:45 2017 |
| 1974 | Thu Nov 30 17:46:49 2017 |
| 1975 | Thu Nov 30 17:46:53 2017 |
| 1976 | Thu Nov 30 17:46:57 2017 |
| 1977 | Thu Nov 30 17:47:02 2017 |
| 1978 | Thu Nov 30 17:47:09 2017 |
| 1979 | Thu Nov 30 17:47:15 2017 |
| 1980 | Thu Nov 30 17:47:22 2017 |
| 1981 | Thu Nov 30 17:47:29 2017 |
| 1982 | Thu Nov 30 17:47:35 2017 |
| 1983 | Thu Nov 30 17:47:42 2017 |
| 1984 | Thu Nov 30 17:47:48 2017 |
| 1985 | Thu Nov 30 17:47:54 2017 |
| 1986 | Thu Nov 30 17:48:02 2017 |
| 1987 | Thu Nov 30 17:48:09 2017 |
| 1988 | Thu Nov 30 17:48:15 2017 |
| 1989 | Thu Nov 30 17:48:22 2017 |
| 1990 | Thu Nov 30 17:48:29 2017 |
| 1991 | Thu Nov 30 17:48:36 2017 |
| 1992 | Thu Nov 30 17:48:43 2017 |
| 1993 | Thu Nov 30 17:48:49 2017 |
| 1994 | Thu Nov 30 17:48:56 2017 |
| 1995 | Thu Nov 30 17:49:02 2017 |
| 1996 | Thu Nov 30 17:49:09 2017 |
| 1997 | Thu Nov 30 17:49:17 2017 |
| 1998 | Thu Nov 30 17:49:24 2017 |
| 1999 | Thu Nov 30 17:49:32 2017 |
| 2000 | Thu Nov 30 17:49:41 2017 |
| 2001 | Thu Nov 30 17:49:48 2017 |
| 2002 | Thu Nov 30 17:50:02 2017 |
| 2003 | Thu Nov 30 17:50:16 2017 |
| 2004 | Thu Nov 30 17:50:31 2017 |
| 2005 | Thu Nov 30 17:50:46 2017 |
| 2006 | Thu Nov 30 17:51:00 2017 |
| 2007 | Thu Nov 30 17:51:15 2017 |
| 2008 | Thu Nov 30 17:51:30 2017 |
| 2009 | Thu Nov 30 17:51:43 2017 |
| 2010 | Thu Nov 30 17:51:56 2017 |
| 2011 | Thu Nov 30 17:52:10 2017 |
| 2012 | Thu Nov 30 17:52:24 2017 |
| 2013 | Thu Nov 30 17:52:39 2017 |
| 2014 | Thu Nov 30 17:52:54 2017 |
| 2015 | Thu Nov 30 17:53:16 2017 |
| 2016 | Thu Nov 30 17:53:31 2017 |
| 2017 | Thu Nov 30 17:53:46 2017 |

then, we use some R code to put the preceding two outputs together and
compute the number of seconds per person.

#+name: peoplepersecond
#+BEGIN_SRC R :var yeartimes=yeartimes :var yearpeople=yearpeople :eval no-export :exports none
  rownames(yearpeople) <- yearpeople[,1]
  colnames(yearpeople) <- c("pyear", "people")
  rownames(yeartimes) <- yeartimes[,1]
  colnames(yeartimes) <- c("tyear", "stime")
  years <- cbind(yearpeople, yeartimes)
  years <- cbind(years, time=as.POSIXct(years$stime, format="%a %b %d %H:%M:%S %Y"))
  deltas <- years[1:nrow(years)-1,]$people /
    max(1, lag(as.ts(years$time))-as.ts(years$time))
  years <- cbind(years, delta=c(deltas, NA))
  cbind(year=years$tyear, perperson=years$delta)
#+END_SRC

so, the number of people processed per second is:

#+RESULTS: peoplepersecond
| 1962 | 3260.95454545455 |
| 1963 | 2540.09090909091 |
| 1964 | 2479.22727272727 |
| 1965 | 2477.36363636364 |
| 1966 |           5002.5 |
| 1967 | 3121.63636363636 |
| 1968 | 6859.68181818182 |
| 1969 | 6902.18181818182 |
| 1970 | 6591.95454545455 |
| 1971 | 6673.72727272727 |
| 1972 | 6383.27272727273 |
| 1973 | 6191.86363636364 |
| 1974 | 6058.27272727273 |
| 1975 | 5914.72727272727 |
| 1976 | 6152.31818181818 |
| 1977 | 7309.04545454545 |
| 1978 | 7077.54545454545 |
| 1979 | 7026.95454545455 |
| 1980 | 8249.45454545455 |
| 1981 | 8243.54545454545 |
| 1982 | 7395.59090909091 |
| 1983 |           7392.5 |
| 1984 | 7325.77272727273 |
| 1985 | 7334.63636363636 |
| 1986 | 7166.40909090909 |
| 1987 | 7066.72727272727 |
| 1988 |             7090 |
| 1989 | 6576.68181818182 |
| 1990 | 7185.40909090909 |
| 1991 |           7203.5 |
| 1992 | 7081.63636363636 |
| 1993 | 7054.40909090909 |
| 1994 | 6861.04545454545 |
| 1995 | 6801.90909090909 |
| 1996 | 5930.72727272727 |
| 1997 | 5993.36363636364 |
| 1998 | 5982.59090909091 |
| 1999 | 6014.72727272727 |
| 2000 | 6077.72727272727 |
| 2001 | 9921.31818181818 |
| 2002 | 9873.59090909091 |
| 2003 | 9837.45454545455 |
| 2004 | 9692.77272727273 |
| 2005 | 9574.90909090909 |
| 2006 | 9480.09090909091 |
| 2007 | 9392.68181818182 |
| 2008 |             9382 |
| 2009 | 9450.95454545455 |
| 2010 | 9536.45454545455 |
| 2011 | 9317.40909090909 |
| 2012 | 9154.45454545455 |
| 2013 | 9210.63636363636 |
| 2014 | 9070.72727272727 |
| 2015 | 9046.54545454545 |
| 2016 | 8431.22727272727 |
| 2017 |              nil |

** code :noexport:
*** stats.binned

we need a summary routine for binned objects.  each bin has a "value"
as well as a number of elements with that value.  we compute the same
objects as summary(): Min, 1st Qu., Median, Mean, 3rd Qu., Max

the input is a matrix with 2 columns, the first being the value, the
second the number of elements with that value.

#+name: stats.binned
#+BEGIN_SRC R :results none :eval no-export :exports none
  <<warning>>

  require(Hmisc, warn.conflicts=FALSE, quietly=TRUE)

  check.binned <- function(fname, vals, nobs) {
    if (length(vals) == 0) {
      stop(sprintf("%s: no values", fname))
    } else if (length(nobs) == 0) {
      stop(sprintf("%s: no observations", fname))
    } else if (length(vals) != length(nobs)) {
      stop(sprintf("%s: length(values) [%d] != length(number of observations) [%d]",
                   fname, length(vals), length(nobs)))
    } else if (!is.numeric(vals[!is.na(vals)])) {
      stop(sprintf("%s: values must be numeric", fname))
    } else if (!is.numeric(nobs[!is.na(nobs)])) {
      stop(sprintf("%s: number of observations must be numeric", fname))
    }
  }

  summary.binned <- function(vals, nobs) {
    if ((length(vals[!is.na(vals)]) == 0) ||
        (length(nobs[!is.na(nobs)]) == 0)) { # "||"? i'm not sure
      return(c(Min=NA, "1st Qu."=NA, Median=NA, Mean=NA, "3rd Qu."=NA, "Max."=NA))
    }
    check.binned("summary.binned", vals, nobs)

    result <- wtd.quantile(x=vals, weights=nobs,
                           probs=c(0, .25, .5, 0, .75, 1))
    names(result) <- c("Min.", "1st Qu.", "Median", "Mean", "3rd Qu.", "Max.")
    result["Mean"] <- wtd.mean(x=vals, weights=nobs)
    return(result)
  }

  rebin.binned <- function(vals, nobs, newvals, ordered=FALSE) {
    "given a set of values, with its set of observations counts,
  produce a new set of bins, with a new set of observation counts.
  the old values vals must fit \"integrally\" into the new vals.
  returns the new observation counts."
    ## only care about actual observed outcomes (and, this makes it
    ## easier to have newvals technically smaller than max(vals), in
    ## case where max(vals) is not actually an observed value).
    vals <- vals[nobs!=0]
    nobs <- nobs[nobs!=0]
    if (!ordered) {
      path <- order(vals)
      newvals <- newvals[order(newvals)]
    } else {
      path <- 1:length(vals)
    }
    if (length(vals) != length(nobs)) {
      stop(sprintf("rebin.binned: length(vals) [%d] != length(nobs) [%d]",
                    length(vals), length(nobs)))
    }
    if (vals[length(vals)] > newvals[length(newvals)]) {
      stop(sprintf("rebin.binned: largest current observed bin (%d) greater than largest new bin (%d)",
                   vals[length(vals)], newvals[length(newvals)]))
    }
    j <- 1                              # index into newvals
    rval <- integer()                   # initialize return value
    count <- 0                          # intialize count (rval element)
    for (i in path) {
      if (vals[i] > newvals[j]) {         # we're in a new bucket
        rval <- c(rval, count)            # so, finish out the previous bucket
        toskip <- sum(vals[i] > newvals[j:length(newvals)])
        count <- 0                        # reinitialize count
        rval <- c(rval, rep(0, toskip-1)) # we may have quite a way to go
        j <- j+toskip                     # fast forward
      }
      count <- count + nobs[i]
    }
    rval <- c(rval, count)                # get last count
    ## fill out rval
    rval <- c(rval, rep(0, length(newvals)-length(rval)))
    rval                                  # return value
  }


  rebinvals.binned <- function(limita, limitb=NA, binsize) {
    "return the set of new values for a given new BINSIZE.  
    can specify MIN and MAX, or just pass the set of observations
    and the new min, max, will be computed."
    min <- min(c(limita, limitb), na.rm=TRUE)
    max <- max(c(limita, limitb), na.rm=TRUE)
    lo <- (floor(min/binsize)+1)*binsize
    hi <- (floor(max/binsize)+1)*binsize
    return(seq(lo, hi, binsize))
  }

  test.rebin.binned <- function() {
    "some unit tests"
    ## basic functionality works?
    if (!identical(rebin.binned(c(1:20), seq(1,20), seq(2,20,by=2)),
                   seq(3, 39, 4))) {
      stop("test.rebin.binned: verification failed")
    }

    ## what if old bin had something too big, but unobserved?
    if (!identical(rebin.binned(c(1:21), c(seq(1,20),0), seq(2,20,by=2)),
                   seq(3, 39, 4))) {
      stop("test.rebin.binned: verification failed")
    }
  }


  test.rebinvals.binned <- function() {
    "trivial unit test for rebinvals.binned; built around
    for (i in c(-6:6, 24:29)) print(rebinvals.binned(i:29, binsize=5))"
    testpat <- list(
                    list(-6, c(-5, 0, 5, 10, 15, 20, 25, 30)),
                    list(-5, c(0, 5, 10, 15, 20, 25, 30)),
                    list(-4, c(0, 5, 10, 15, 20, 25, 30)),
                    list(-3, c(0, 5, 10, 15, 20, 25, 30)),
                    list(-2, c(0, 5, 10, 15, 20, 25, 30)),
                    list(-1, c(0, 5, 10, 15, 20, 25, 30)),
                    list(0, c(5, 10, 15, 20, 25, 30)),
                    list(1, c(5, 10, 15, 20, 25, 30)),
                    list(2, c(5, 10, 15, 20, 25, 30)),
                    list(3, c(5, 10, 15, 20, 25, 30)),
                    list(4, c(5, 10, 15, 20, 25, 30)),
                    list(5, c(10, 15, 20, 25, 30)),
                    list(6, c(10, 15, 20, 25, 30)),
                    list(24, c(25, 30)),
                    list(25, c(30)),
                    list(26, c(30)),
                    list(27, c(30)),
                    list(28, c(30)),
                    list(29, c(30)))
    for (x in testpat) {
      i <- x[[1]]
      z <- x[[2]]
      zz <- rebinvals.binned(i:29, binsize=5)
      if (!identical(z, zz)) {
        print(z); print(zz);
      }
    }
  }
#+END_SRC

*** bincps
   :PROPERTIES:
   :ORDERED:  t
   :END:

in our file, the HHINCOME column is replaced by a (computed)
HHINCOME1999: the reported HHINCOME in 1999 dollars.  this is so bins
are comparable between years.  we use IPUMS' CPI99 column for this
purpose.

then, what we want is create a file which is a "binned" version of the
full-detail file which, instead of the detail file's HHINCOME column,
will have a HHBRACKET99, which will include all data with HHINCOME99
in the same "bracket" ("bin"), of $1,000, say.  this involves "rolling
up" the [H]WTSUPP columns by year, dropping the SERIAL, CPSID, PERNUM,
CPSIDP columns in the process.  (additionally, the MONTH column may be NA'd, if there is more
than one month in a bin -- unlikely, given that the releases seem to
be in March of every year.)

run something like this:
: ./bincps --ifile ipums/cps_00006.csv.gz --ofile foo.csv --rfile goo.csv

#+name: bincps
#+BEGIN_SRC R :tangle bin/bincps :shebang "#!/usr/bin/env Rscript" :results none :eval no-export :exports none
  <<warning>>
  <<stats.binned>>

  getout <- function(message, code) {
    if (interactive()) {
      stop(message)
    } else {
      cat(message)
      quit(save="no", status=code)
    }
  }


  bincps <- function(ifile,      # input file
                     ofile="",   # output csv file ("" ==>
                                          # compute from ifile)
                     ofsep="-",  # separator (when ofile or rfile blank)
                     rfile="",   # output report file (see ofile)
                     fyear=-Inf, # first year to include
                     lyear=Inf,  # last year to include
                     min99=-Inf, # minimum HH{INCOME,BRACKET}99 (in USD)
                     max99=Inf,  # maximum HH{INCOME,BRACKET}99 (in USD)
                     ## things < min99, > max99 are included in the
                     ## smallest and largest bins; NA are not included
                     binsize=1000,    # size of bins
                     trimends=TRUE,   # don't output out of range income
                     infminmax=FALSE, # label too small -Inf, too large Inf?
                     verbose=1        # how verbose to be
                     ) {
    if (verbose > 0) {
      cat(sprintf("about to read.csv %s\n", date()))
    }
    dset <<- read.csv(ifile, header=TRUE)
    if (verbose > 0) {
      cat(sprintf("done with read.csv %s\n", date()))
    }
    if (nrow(dset) == 0) {
      getout(sprintf("no data in dataset \"%s\"\n", ifile), 1)
    }

    ## get rid of records outside our years of interest (fyear, lyear)
    if ((fyear != -Inf) || (lyear != Inf)) {
      dset <- dset[dset$YEAR >= fyear & dset$YEAR <= lyear,]
    }

    if (nrow(dset) == 0) {
      getout(sprintf("no data in dataset \"%s\" for years between %g and %g\n",
                     ifile, fyear, lyear), 1)
    }

    ## now, make min99, max99 multiples of binsize
    if (!is.infinite(min99)) {
      min99 <- (min99%/%binsize)*binsize
    }
    if (!is.infinite(max99)) {
      max99 <- (((max99-1)%/%binsize)*binsize)+binsize
    }

    ## now, check if output files are okay
    orlabel <- sprintf("%d%s%d", min(dset$YEAR), ofsep, max(dset$YEAR))
    ofto <- ofsep
    if (min99 != -Inf) {
      orlabel <- sprintf("%s%s%d", orlabel, ofsep, min99);
      ofto <- sprintf("%sto%s", ofsep, ofsep)
    }
    if (max99 != Inf) {
      orlabel <- sprintf("%s%sto%s%d", orlabel, ofsep, ofsep, max99);
      ofto <- ofsep
    }
    orlabel <- sprintf("%s%sbinned", orlabel, ofto)
    rrlabel <- sprintf("%s%sreport", orlabel, ofsep)
    ofile <- dealwithoutputfilename(ifile, ofile, "output", orlabel)
    rfile <- dealwithoutputfilename(ifile, rfile, "report", rrlabel)

    ## we may be running on "raw" (via ipums) census data, or we may be
    ## looking at output of a previous run (already binned).  which is it?
    if (is.element("HHINCOME", colnames(dset))) {
      income99 <- "HHINCOME99"
      ## now, convert all income to 1999 dollars
      dset <- cbind(dset,
                    HHINCOME99=dset$HHINCOME*dset$CPI99, # normalize to 1999 dollars
                    NRESP=1)              # number of responses
    } else if (is.element("HHBRACKET99", colnames(dset))) {
      income99 <- "HHBRACKET99"
      ## what is input binsize?  to figure this out, we look at the
      ## smallest difference between successive HHBRACKET99's
      x <- dset$HHBRACKET99                   # brackets
      y <- unique(c(x[2:length(x)], NA) - x) # list of unique deltas + NA
      ibsize <- min(abs(y), na.rm=TRUE)   # take min, ignoring NA
      if (is.na(ibsize)) {
        getout(sprintf("unable to compute input binsize of input file \"%s\"\n",
                       ifile), 1)
      }
      ## now, is the input binsize a divisor of the desired output binsize?
      if ((ibsize%%binsize) != 0) {
        getout(sprintf("the input file appears to have a binsize of %d, but the desired binsize %d is not a multiple of this\n",
                       ibsize, binsize), 1)
      }
    } else {
      getout("bincps: input has neither HHINCOME (raw) or HHBRACKET99 (output of previous run\n", 1)
    }

    rval <- bincps1(dset=dset,
                    min99=min99,
                    max99=max99,
                    binsize=binsize,
                    trimends=trimends,
                    infminmax=infminmax,
                    verbose=verbose,
                    income99=income99)

    bset <- rval$bset
    rset <- rbind(rval$rsetun, rval$rsethwt, rval$rsetwt)
    write.csv(rval$bset, ofile, row.names=FALSE, quote=FALSE);
    if (nrow(rset) != 0) {                # anything to report?
      ## if so, first sort it, then write it out
      rset <- rset[order(rset$YEAR, rset$"Max."),]
      write.csv(rset, rfile, row.names=FALSE, quote=FALSE)
    }
  }

  bincps1 <- function(dset,                # inherits other locals from
                      min99=-Inf, # minimum HH{INCOME,BRACKET}99 (in USD)
                      max99=Inf, # maximum HH{INCOME,BRACKET}99 (in USD)
                      ## things < min99, > max99 are included in the
                      ## smallest and largest bins; NA are not included
                      binsize=1000,        # size of bins
                      trimends=TRUE,
                      infminmax=FALSE,     # should min/max bins be
                                           # labelled "[-]Inf"?  if this
                                           # is FALSE, < min99 will go
                                           # just below the lowest bin,
                                           # and >= max99 will go just
                                           # above the highest bin
                      verbose=1,
                      income99
                      ) {
    ## get *all* the bins...
    dset <- cbind(dset, BRACKET=(floor(dset[,income99]/binsize)*binsize)+binsize)

    ## this is in lieu of a macro facility in R (or in lieu of <<noweb>>
    ## working in org-mode when running code via C-c C-c).  this routine
    ## is called to enter rows into the output table (and, can access --
    ## read and write -- our variables from the calling routine)
    ahroutine <- function(filter, bracket) {
      if (verbose > 1) {
        cat(sprintf("ahroutine, year %d, nrow filter %d, bracket %g, nrow bset %d\n",
                      year, nrow(yset[filter,]), bracket, nrow(bset)))
      }
      for (asecflag in unique(yset[filter,]$ASECFLAG)) {
        if (!is.na(asecflag)) {
          sa <- filter & yset$ASECFLAG == asecflag
        } else {
              sa <- filter & is.na(yset$ASECFLAG)
        }
        for (hflag in unique(yset[sa,]$HFLAG)) {
          if (!is.na(hflag)) {
            sh <- sa & yset$HFLAG == hflag
          } else {
            sh <- sa & is.na(yset$HFLAG)
          }
          if (nrow(yset[sh,]) != 0) {
            ## *finally* -- do something!
            month <- unique(yset[sh,]$MONTH)
            if (length(month) > 1) {
              month <- NA
            }
            cpi99 <- unique(yset[sh,]$CPI99)
            if (length(cpi99) > 1) {
              cpi99 <- NA
            }
            bset <<- rbind(bset,
                           data.frame(YEAR=year,
                                      HWTSUPP=sum(yset[sh,]$HWTSUPP),
                                      ASECFLAG=asecflag,
                                      HFLAG=hflag,
                                      HHBRACKET99=bracket,
                                      CPI99=cpi99,
                                      MONTH=month,
                                      WTSUPP=sum(yset[sh,]$WTSUPP),
                                      NRESP=sum(yset[sh,]$NRESP)))
          }
        }
      }
    }

    mysummary <- function(vals, nobs=NULL) {
      "like summary, but try for a format consistent across numbers, NA, ..."
      if (is.null(nobs)) {
        nobs <- rep(1, length(vals))
      }
      summary <- summary.binned(vals, nobs);
      if("NA's" %in% names(summary)) {
        summary <- summary[-which(names(summary) == "NA's")]
      }
      ## make names consistent (else rbind() complains)
      names(summary) <- c("Min.", "1st Qu.", "Median", "Mean", "3rd Qu.", "Max.")
      return(summary)
    }

    ## deal with execptional data, i.e., data that is either
    ## NA-contaminated, or data that is outside the min99/max99 bounds
    rsetting <- function(filter, comment) {
      commentun <- sprintf("(unweighted) %s", comment)
      commenthwt <- sprintf("(hwtsupp-weighted) %s", comment)
      commentwt <- sprintf("(wtsupp-weighted) %s", comment)
      rsetun <<- rbind(rsetun,
                       data.frame(t(c(YEAR=year,
                                      HWTSUPP=sum(yset[filter,]$HWTSUPP),
                                      WTSUPP=sum(yset[filter,]$WTSUPP),
                                      mysummary(yset[filter,income99]),
                                      COMMENT=commentun))))
      rsethwt <<- rbind(rsethwt,
                       data.frame(t(c(YEAR=year,
                                      HWTSUPP=sum(yset[filter,]$HWTSUPP),
                                      WTSUPP=sum(yset[filter,]$WTSUPP),
                                      mysummary(yset[filter,income99],
                                                yset[filter,]$HWTSUPP),
                                      COMMENT=commenthwt))))
      rsetwt <<- rbind(rsetwt,
                       data.frame(t(c(YEAR=year,
                                      HWTSUPP=sum(yset[filter,]$HWTSUPP),
                                      WTSUPP=sum(yset[filter,]$WTSUPP),
                                      mysummary(yset[filter,income99],
                                                yset[filter,]$WTSUPP),
                                      COMMENT=commentwt))))
    }

    ## the binned data goes here
    bset <- data.frame()
    ## three data frames for exception reporting.  the first is
    ## unweighted "income99" (HHINCOME99 or HHBRACKET99, as the case may
    ## be); the second weighted by HWTSUPP; and the third by WTSUPP.
    rsetun <- data.frame()
    rsethwt <- data.frame()
    rsetwt <- data.frame()
    for (year in sort(unique(dset$YEAR))) {
      yset <- dset[dset$YEAR == year,]
      sy <- TRUE                          # initially, take all in this year
      if (verbose > 0) {
        cat(sprintf("%s %s\n", year, date()))
      }

      ## get rid of out of universe, etc., codes
      ## https://cps.ipums.org/cps/inctaxcodes.shtml
      if ("HHINCOME" %in% colnames(yset)) {
        stopbit <- yset[,"HHINCOME"] %in% c(-9999997, -9999, 9999, 99990, 99991, 99992, 99994, 99995, 99996, 99997, 99998, 99999, 99999999)
        stop <- sy & stopbit
        if (nrow(yset[stop,]) != 0) {
          rsetting(stop, "topcodes (Census Bureau/IPUMS coded as invalid)")
          sy <- sy & !stopbit             # get rid of these
        }
      }

      snabit <- is.na(yset[,income99])
      sna <- sy & snabit
      if (nrow(yset[sna,]) != 0) {
        ahroutine(sna, NA)                # enter (these) row(s)
        rsetting(sna, "income not provided")
        sy <- sy & !snabit                # now, kill them
      }

      ## describe and enter the too small incomes
      slowbit <- yset[,income99] < min99
      slow <- sy & slowbit
      if (nrow(yset[slow,]) != 0) {
        if (!trimends) {                  # should we describe these?
          ## enter (these) row(s)
          if (!infminmax) {
            ahroutine(slow, min99)
          } else {
            ahroutine(slow, -Inf)
          }
        }
        rsetting(slow, sprintf("less than %d", min99))
        sy <- sy & !slowbit               # now, kill them
      }

      ## now, describe too high incomes (and then enter them below)
      shighbit <- yset[,income99] >= max99
      shigh <- sy & shighbit
      if (nrow(yset[shigh,]) != 0) {
        rsetting(shigh, sprintf("greater than or equal to %d", max99))
        sy <- sy & !shighbit              # now, kill them
      }

      ## we don't describe *other* bins since they are of limited bracket;
      ## the "negative" and "greater than max" bins are not of an a
      ## priori known limit.

      ## now, add all the bins (if there are any!)
      uy <- unique(yset[sy,]$BRACKET)
      if (!is.null(uy)) {
        for (bin in sort(uy)) {
          sb <- sy & yset$BRACKET == bin
          ahroutine(sb, bin)
        }
      }

      ## now, add too high
      if (nrow(yset[shigh,]) != 0) {
        if (!trimends) {
          ## enter (these) row(s)
          if (!infminmax) {
            ahroutine(shigh, max99+binsize)
          } else {
            ahroutine(shigh, Inf)
          }
        }
      }
    }
    return(list(bset=bset, rsetun=rsetun, rsethwt=rsethwt, rsetwt=rsetwt))
  }

  ## if necessary, cons up an appropriate FNAME.  then, checks that
  ## FNAME doesn't already exist and that it is (potentially) writeable.

  ## NB: as a side effect of testing writeability, on a successful
  ## return, FNAME *will* exist (but, be empty).
  dealwithoutputfilename <- function(ifile, fname, use, lastbits) {
    require(assertthat, quietly=TRUE)     # XXX still needed?

    if (is.na(fname)) {                    # compute filename
      x <- strsplit(ifile, ".", fixed=TRUE)[[1]]
      if (x[length(x)] == "gz") {
        length(x) = length(x)-1           # get rid of .gz (we don't compress)
      }
      x[length(x)] <- sprintf("%s.%s", lastbits, x[length(x)]);
      fname <- paste(x, collapse=".")
    }

    ## test if already exists (a no-no)
    if (file.exists(fname)) {
      getout(sprintf("%s file \"%s\" exists, won't overwrite\n", use, fname), 2)
    }

    ## test if writeable (better be!)
    failed <- FALSE;
    x <- tryCatch(file(fname, "w"), 
                  error=function(e) failed <<- TRUE);
    if (failed) {
      getout(sprintf("%s file \"%s\" is not writeable\n", use, fname), 2)
    }
    close(x)

    return(fname)
  }

  main <- function(args=NULL) {
    require(argparser, quietly=TRUE)

    p <- arg_parser("bincps")
    p <- add_argument(p, "--ifile", type="character", default=NA,
                      help="input data (.csv or .csv.gz) file")
    p <- add_argument(p, "--ofile", type="character", default=NA,

                      help="output data file; if not specified, an automatically generated name will be used")
    p <- add_argument(p, "--rfile", type="character", default=NA,
                      help="output exception report file; if not specified, an automatically generated name will be used")
    p <- add_argument(p, "--ofsep", type="character", default="-",
                      help="separator used when automatically generating ofile, rfile names")
    p <- add_argument(p, "--fyear", type="integer", default=-Inf,
                      help="first year to process; if not specified, the first year in the input file will be used")
    p <- add_argument(p, "--lyear", type="integer", default=Inf,
                      help="last year to process; if not specified, the last year in the input file will be used")
    p <- add_argument(p, "--binsize", type="integer", default=1000,
                      help="output bin size")
    p <- add_argument(p, "--min99", type="integer", default=-Inf, short="-m",
                      help="don't bin dollar amounts below this value")
    p <- add_argument(p, "--max99", type="integer", default=Inf, short="-M",
                      help="don't bin dollar amounts above this value")
    p <- add_argument(p, "--verbose", type="integer", default=0,
                      help="informational/debugging output quantity")
    p <- add_argument(p, "--trimends", flag=TRUE, default=TRUE,
                      help="should < MIN99 and > MAX99 be left out of output?")
    p <- add_argument(p, "--infminmax", flag=TRUE, default=FALSE,
                      help="should bins for values below min99 (resp. above max99) appear as \"-Inf\" (resp. \"Inf\"); if not, they will be assigned bins just below min99 (resp. just above max99)")

    if (is.null(args)) {
      argv <- parse_args(p)
    } else {
      argv <- parse_args(p, args)
    }

    bincps(ifile=argv$ifile,
           ofile=argv$ofile,
           rfile=argv$rfile,
           ofsep=argv$ofsep,
           fyear=argv$fyear,
           lyear=argv$lyear,
           binsize=argv$binsize,
           min99=argv$min99,
           max99=argv$max99,
           verbose=argv$verbose,
           trimends=argv$trimends,
           infminmax=argv$infminmax);
  }

  runargs <- function(ifile,      # input file
                      ofile=NA,   # output csv file ("" ==>
                                          # compute from ifile)
                      ofsep="-",  # separator (when ofile or rfile blank)
                      rfile=NA,   # output report file (see ofile)
                      fyear=-Inf, # first year to include
                      lyear=Inf,  # last year to include
                      min99=-Inf, # minimum HH{INCOME,BRACKET}99 (in USD)
                      max99=Inf, # maximum HH{INCOME,BRACKET}99 (in USD)
                      ## things < min99, > max99 are included in the
                      ## smallest and largest bins; NA are not included
                      binsize=1000,        # size of bins
                      infminmax=FALSE,     # label too small -Inf, too large Inf?
                      verbose=1            # how verbose to be
                      ) {
    cmdline <- c("--ifile", ifile,
                 "--verbose", verbose)

    main(cmdline)
  }

  options(error=recover)
  options(warn=2)
  # debug(bincps1)



  if (!interactive()) {
    main()
    print(warnings())
  }
#+END_SRC

*** looking at the data
**** common code
the following code is common to all our visualizations.  we break it
down in pieces
***** all the bits, nothing but the bits
#+name: looking.bits
#+BEGIN_SRC R :results none :eval no-export :exports none :var ifile=ifile
  <<warning>>
  <<looking.dset>>
  <<looking.parts>>
  <<looking.create.lookup>>
  <<looking.parser>>
  <<looking.graph>>
#+END_SRC

# the bits we need for most reported incomes
#+name: reported.bits
#+BEGIN_SRC R :results none :eval no-export :exports none :var ifile=ifile
  <<warning>>
  <<looking.parser>>
  <<looking.graph>>
  <<reported.dset>>
  <<mostreported>>
#+END_SRC


get the dataset
#+name: looking.dset
#+BEGIN_SRC R :results none :eval no-export :exports none :var ifile=ifile
  looking.tidyup <- function(nvals, nobs, maxval) {
    "take the input dataset, cap it"
    ## now, clamp upper end at maxval (200,000, say)
    ## first, propagate counts of the higher incomes to end of chart
    highest <- max(nvals[nvals <= maxval], na.rm=TRUE)
    if (!is.numeric(highest)) {
      stop(sprintf("looking.tidyup: all values greater than maxval (%g)", maxval))
    }
    nobs[nvals==highest] <- nobs[nvals==highest] + sum(nobs[nvals>highest])
    ## now, delete the higher values
    slowenough <- nvals <= maxval
    nobs <- nobs[slowenough]
    nvals <- nvals[slowenough]
    ## what about too low?  assume 0
    lowest <- min(nvals[nvals >= 0], na.rm=TRUE)
    if (!is.numeric(lowest)) {
      stop(sprintf("looking.tidyup: all values less than maxval (%g) also less than zero",
                   maxval))
    }
    ## count them, and get rid of them
    nobs[nvals == lowest] <- nobs[nvals==lowest] + sum(nobs[nvals<0])
    shighenough <- nvals >= 0
    nobs <- nobs[shighenough]
    nvals <- nvals[shighenough]
    ## get rid of never-observed values, if any (efficiency)
    sn0 <- nobs != 0
    nvals <- nvals[sn0]
    nobs <- nobs[sn0]
    ## data frame it
    df <- data.frame(vals=nvals, nobs=nobs)
  }

  looking.dset <- function(argv) {        # command line args
    dset <- read.csv(argv$ifile, header=TRUE)
    dset <- dset[dset$HFLAG == FALSE | is.na(dset$HFLAG),]
    dset <- dset[dset$YEAR>=min(argv$years),]  # before 1967, NA data

    min <- min(dset[, argv$bracket], na.rm=TRUE)
    max <- max(dset[, argv$bracket], na.rm=TRUE)

    for (year in unique(dset$YEAR)) {
      sy <- dset$YEAR==year
      yset <- dset[dset$YEAR==year,]
      tidy <- looking.tidyup(dset[sy,argv$bracket], dset[sy,argv$supp], argv$maxhh)
      ## now, get rid of outliers (looking.tidyup included in 0, maxhh indicies)
      dset <- dset[-which(sy & (dset[,argv$bracket] < 0 |
                                dset[,argv$bracket] > argv$maxhh)),]
      sy <- dset$YEAR==year
      if (!all(dset[sy,argv$bracket] == tidy$vals)) {
        stop(sprintf("internal error"))
      }
      dset[sy,argv$supp] <- tidy$nobs
    }
    ## now, remember parameters in attributes
    for (a in names(argv)) {
      if (a != "") {
        attr(dset, a) <- argv[[a]]
      }
    }
    dset
  }
#+END_SRC

#+name: looking.parts
#+BEGIN_SRC R :results none :eval no-export :exports none :var ifile=ifile
  require(ggplot2, quietly=TRUE)
  require(assertthat, quietly=TRUE)

  ## the following work on named part(s)
  part.pct <- function(part) {
      part$pct
  }

  part.base.subr <- function(str) {
      gsub("^%", "", str)
  }

  part.base <- function(part) {
      part.base.subr(part$name)
  }

  part.desc.subr <- function(str, pct) {
      rval <- switch(part.base.subr(str),
                     "bracket" = "income brackets",
                     "totalincome" = "total income",
                     "population" = "population",
                     stop("programming error"))
      if (pct) {
          rval <- sprintf("fraction of %s", rval)
      }
      rval
  }

  part.desc <- function(part) {
      part.desc.subr(part$name, part$pct)
  }

  part.max <- function(dset, years, part) {
      if (part$pct) {
          return(1)                       # easy!
      }
      yset <- dset[dset$YEAR %in% years,]
      switch(part.base(part),
             "bracket" = max(yset[,attr(dset, "bracket")]),
             "totalincome" = sum(yset[, attr(dset, "bracket")]*
                                 yset[, attr(dset, "supp")]),
             "population" = sum(yset[, attr(dset, "supp")]))
  }

  part.create <- function(str) {
      pct <- grepl("^%.*$", str)
      list(name=str, pct=pct,
           desc=part.desc.subr(str, pct))
  }

  ## this *creates* named parts
  tgt.in.key <- function(tgt.in.key) {
      parts <- unlist(strsplit(tgt.in.key, ":")) # get bits of tgt.in.key
      if (length(parts) != 2) {
          stop(sprintf("unknown/improperly formatted tgt.in.key \"%s\"", tgt.in.key))
      }
      list(tgt=part.create(parts[[1]]),
           key=part.create(parts[[2]]))
  }

  parts.title <- function(parts) {
      "cumulative distribution [of values] of [totalincome] as a function of increasing [fraction of] [population]"
      sprintf("cumulative distribution%sof %s as a function of increasing%s%s",
              ifelse(parts$tgt$pct, " ", " of values "),
              parts$tgt$desc,
              ifelse(parts$key$pct, " ", " values of "),
              parts$key$desc)
  }

  part.other <- function(part) {
      "if %foo, foo; else %foo"
      rval <- part
      if (part.pct(part)) {
          rval$name <- part.base(part)
          rval$pct <- FALSE
      } else {
          rval$name <- sprintf("%%%s", part.base(part))
          rval$pct <- TRUE
      }
      rval
  }

  parts.two.to.parts <- function(tgt, key) {
      "combine a target and a key part into a tgt.in.key string"
      sprintf("%s:%s", tgt$name, key$name)
  }




  ## looking up:
  ## 1. totalincome:population: given a cum population, what is cum totalincome?
  ## 2. %totalincome:population: given a cum population, what is % totalincome?
  ## 3. totalincome:%population: given a % population, what is cum totalincome?
  ## totalincome:population; totalincome:bracket; totalincome:totalincome
  ## population:totalincome; population:bracket; population:population
  ## bracket:totalincome; bracket:bracket; bracket:population
  ## (where each can be, e.g., "population" or "%population")

  ## lookup(target, by, target%=TRUE, by%=TRUE): given a value (%) of
  ## "by", return corresponding value (%) of target.  target, by \in
  ## {totalincome, bracket, population}.


  cumpart <- function(dset, part) {
      "return a vector of the appropriate bit of the database"
      ## precompute for code legibility
      bracket <- dset[,attr(dset, "bracket")]
      supp <- dset[,attr(dset, "supp")]
      rval <- switch(part.base(part),
                     "totalincome" = bracket*supp,
                     "bracket" = c(bracket[1], diff(bracket)),
                     "population" = supp,
                     stop(sprintf("invalid tgt.in.key \"%s\"", tgt.in.key)))
      if (part.pct(part)) {
          rval <- cumsum(rval)/sum(rval)  # fraction it
      } else {
          rval <- cumsum(rval)
      }
      rval
  }
#+END_SRC

#+name: looking.create.lookup
#+BEGIN_SRC R :results none :eval no-export :exports none :var ifile=ifile
  create.lookup <- function(dset, year, tgt.in.key) {
      "create a lookup function"
      yset <- dset[dset$YEAR==year,]
      ## decode keys
      parts <- tgt.in.key(tgt.in.key)
      key <- cumpart(yset, parts$key)
      tgt <- cumpart(yset, parts$tgt)

      rval <- approxfun(key, tgt,
                        method="constant",
                        yleft = tgt[1], yright = tgt[length(tgt)],
                        f = 0,              # XXX is this the right f for inverse?
                        ties = "ordered")
      attr(rval, "call") <- sys.call()
      rval
  }
#+END_SRC

#+name: looking.parser
#+BEGIN_SRC R :results none :eval no-export :exports none
  ## different utilities need different combinations of (a common set
  ## of) parameters
  doparser <- function(name, need, args) {
    require(argparser, quietly=TRUE)
    # options(error = quote({dump.frames(to.file = TRUE); q(status = 1)}))

    p <- arg_parser(name)
    if ("ifile" %in% need) {
      need <- subset(need, need!="ifile")
      p <- add_argument(p, "--ifile", type="character",
                        default=system("make -s binned", intern=TRUE),
                        help="name of input file")
    }
    if ("reported" %in% need) {
        need <- subset(need, need!="reported");
        p <- add_argument(p, "--reported", type="character",
                          default=system("make -s reported", intern=TRUE),
                          help="name of income frequencies file")
    }
    if ("maxhh" %in% need) {
      need <- subset(need, need!="maxhh")
      p <- add_argument(p, "--maxhh", type="numeric", default=200000,
                        help="cap on household income (larger values will be counted in last bin")
    }
    if ("supp" %in% need) {
      need <- subset(need, need!="supp")
      p <- add_argument(p, "--supp", type="character", default="HWTSUPP",
                        help="column name to use as count")
    }
    if ("bracket" %in% need) {
      need <- subset(need, need!="bracket")
      p <- add_argument(p, "--bracket", type="character", default="HHBRACKET99",
                        help="column name to use as income")
    }
    if ("binsize" %in% need) {
      need <- subset(need, need!="binsize")
      p <- add_argument(p, "--binsize", type="integer", default=1000,
                        help="size of each bin")
    }
    if ("fyear" %in% need) {
      need <- subset(need, need!="fyear")
      p <- add_argument(p, "--fyear", type="integer", default=1968,
                        help="first year of input dataset to process")
    }
    if ("graphics" %in% need) {
      need <- subset(need, need!="graphics")
      p <- add_argument(p, "--graphics", type="character", default="X11",
                        help="medium (\"X11\", \"pdf\", \"png\", \"svg\", etc.) to hold output graph")
    }
    if ("gfile" %in% need) {
      need <- subset(need, need!="gfile")
      p <- add_argument(p, "--gfile", type="character", default=NA,
                        help="output graphics file name\n\t(UNTESTED: for X11, display name)")
    }
    if ("height" %in% need) {
      need <- subset(need, need!="height")
      p <- add_argument(p, "--height", type="numeric", default=NA,
                        help="height of graphic output")
    }
    if ("width" %in% need) {
      need <- subset(need, need!="width")
      p <- add_argument(p, "--width", type="numeric", default=NA,
                        help="width of graphic output")
    }
    if ("silentshrink" %in% need) {
      need <- subset(need, need!="silentshrink")
      p <- add_argument(p, "--silentshrink", type="boolean", flag=TRUE,
                        help="rankboard: shrink font, but do *not* warn, if labels too big to fit")
    }
    if ("rbsqpadding" %in% need) {
      need <- subset(need, need!="rbsqpadding")
      p <- add_argument(p, "--rbsqpadding", type="numeric", default=0.01,
                        help="rankboard: padding around text in square (frac.)")
    }
    if ("keep" %in% need) {
      need <- subset(need, need!="keep")
      p <- add_argument(p, "--keep", type="numeric", default=10,
                        help="rankboard: how many of top incomes to display")
    }
    if ("gopts" %in% need) {
      need <- subset(need, need!="gopts")
      p <- add_argument(p, "--gopts", type="character", default="",
                        help="options specific to --graphics type")
    }
    if ("years" %in% need) {
      need <- subset(need, need!="years")
      p <- add_argument(p, "--years", type="character", default=system("make -s years", intern=TRUE),
                        help="years to graph")
    }
    if ("tiles" %in% need) {
      need <- subset(need, need!="tiles")
      p <- add_argument(p, "--tiles", type="character", default="0.1,0.9",
                        help="percentiles to mark")
    }
    if ("diff" %in% need) {
      need <- subset(need, need!="diff")
      p <- add_argument(p, "--diff", type="boolean", flag=TRUE,
                        help="should \"tiles\" be presented as CDF (default) or PDF")
    }
    if ("keys" %in% need) {
      need <- subset(need, need!="keys")
      p <- add_argument(p, "--keys", type="character",
                        help="keys to look up")
    }
    if ("tgt.in.key" %in% need) {
      need <- subset(need, need!="tgt.in.key")
      p <- add_argument(p, "--tgt.in.key", type="character", default="bracket:%population",
                        help="tgt.in.key of what?  terms are \"[%]population\", \"[%]totalincome\", \"[%]bracket\"")
    }
    if ("delta" %in% need) {
        need <- subset(need, need!="delta")
        p <- add_argument(p, "--delta", type="boolean", flag=TRUE,
                          help="if we should look at difference between (exactly) two years")
    }
    ## detect bad arguments in need
    if (length(need) != 0) {
      print(need)
      stop("programming error: unknown \"need\" values to buildparser()")
    }

    if (is.null(args)) {
      argv <- parse_args(p)
    } else {
      if (length(args) == 1) {
        args <- unlist(strsplit(x=args, split="[ \n]+"))
      }
      argv <- parse_args(p, args)
    }

    if ((!is.null(argv$ifile)) && is.na(argv$ifile)) {
      stop(sprintf("%s: missing --ifile argument", name))
    }

    ## turn space/comma separated values into numeric vector (for
    ## certain arguments)
    for (i in c("years", "tiles", "keys")) {
      if ((!is.null(argv[[i]])) && !is.na(argv[[i]])) {
        argv[i] <- list(eval(parse(text=paste("c(", argv[[i]], ")"))))
      }
    }

    if (!is.null(argv$graphics)) {
      dograph(graphics=argv$graphics,
              gfile=argv$gfile,
              height=argv$height,
              width=argv$width,
              gopts=argv$gopts)
    }

    if (!is.null(argv$ifile)) {
        return(looking.dset(argv))
    } else if (!is.null(argv$reported)) {
        return(reported.dset(argv))
    }
  }
#+END_SRC


#+name: looking.graph
#+BEGIN_SRC R :results none :eval no-export :exports none :var ifile=ifile
  dograph <- function(graphics, gfile, height, width, gopts) {
    "point our graphic output at a particular device (X11, pdf, png, svg)"
    topts <- function(name, value) {
      if (is.character(value)) {
        value <- sprintf("\"%s\"", value)
      }
      sub("[ ,]*$", "",
          gsub("  *", " ",
               paste(
                     sprintf("%s=%s", name, value),
                     gopts, sep=",")))
    }
    lgraphics <- tolower(graphics)
    cmd <- lgraphics                      # default
    if (!is.na(height)) {
      gopts <- topts("height", height)
    }
    if (!is.na(width)) {
      gopts <- topts("width", width)
    }
    if (lgraphics == "x11") {
      ## get X11 running: https://stackoverflow.com/a/8168190
      if (!is.na(gfile)) {
        gopts <- topts("display", gfile)
      }
    } else if (lgraphics == "pdf") {
      if (!is.na(gfile)) {
        gopts <- topts("file", gfile)
      }
    } else if (lgraphics == "png") {
      if (!is.na(gfile)) {
        gopts <- topts("file", gfile)
      }
    } else if (lgraphics == "svg") {
      if (!is.na(gfile)) {
        gopts <- topts("file", gfile)
      }
    } else {
      stop(sprintf("unknown --graphics option value \"%s\"; known values: \"x11\", \"pdf\", \"png\", \"svg\" (case independent)",
                   graphics))
    }
    eval(parse(text=paste(cmd, "(", gopts, ")", sep="")))

  }

  endgraph <- function(graphics, gopts) {
      "after processing, close the graphics device"
    if (grepl("^x11$", graphics, ignore.case=TRUE)) {
        require(grid, quietly=TRUE)
        cat("click on the graph to end:")
        try(grid.locator(), silent=TRUE)  # in case window manager closes window
        cat("\n")
    }
    try(dev.off(), silent=TRUE)           # ditto
  }


  endup <- function(dset) {
    if (!is.null(attr(dset, "graphics"))) {
      endgraph(attr(dset, "graphics"), attr(dset, "gopts"))
    }
  }
#+END_SRC

#+name: looking.endbits
#+BEGIN_SRC R :eval no-export :exports none
  options(warn=2)

  ## something weird between org-mode and ess.
  ## "if (interactive) { options(error=recover) }"
  ## crashes
  if (interactive()) {
      oerror <- recover;
  } else {
      oerror <- NULL
  }
  options(error=oerror)

  if (!interactive()) {
    rval <- main()
    if (length(warnings()) != 0) {
      print(warnings())
    }
  }
#+END_SRC

**** create.ecdf.common (called by shell scripts)

#+BEGIN_SRC R :tangle bin/create.ecdf.common :shebang "#!/usr/bin/env Rscript" :results none :eval no-export :exports none
  <<warning>>
  <<looking.bits>>
  <<stats.binned>>


  ## http://www.cookbook-r.com/Graphs/Legends_(ggplot2)/
  ## http://www.cookbook-r.com/Graphs/Axes_(ggplot2)
  ggp.ecdf <- function(dset, years, tgt.in.key, delta) {
    parts <- tgt.in.key(tgt.in.key)
    title <- parts.title(parts)
    if (delta) {
        other.tik <- parts.two.to.parts(part.other(parts$tgt), parts$key)
        other.parts <- tgt.in.key(other.tik)
        maxy <- max(years)
        miny <- min(years)
        title <- sprintf("Allocation of change in %s between %s and %s",
                         parts$tgt$desc, maxy, miny)
        if (length(years) != 2) {
            stop(sprintf("with --delta, exactly two years must be specified; instead %d were",
                         length(years)))
        }
        maxy.ecdf <- create.lookup(dset, maxy, other.tik)
        miny.ecdf <- create.lookup(dset, miny, other.tik)
        ## okay, in tgt.in.key, assume key is "%"
        if (!part.pct(parts$key)) {
            stop(sprintf("with \"--delta\", need the key in \"%s\" to begin with '%%'",
                         tgt.in.key))
        }
        sequence <- seq(from=0.1, to=1.0, by=0.1)
        deltas <- vapply(sequence, function(x) maxy.ecdf(x)-miny.ecdf(x),1)
        if (part.pct(parts$tgt)) {
            deltas <- deltas/sum(deltas)
        }
        toplot <- data.frame(x=sequence, y=deltas)
        base <- ggplot(toplot, aes(x=x, y=y)) +
            ggtitle(title) +
            xlab(parts$key$desc) +
            ylab(parts$tgt$desc) +
            scale_x_continuous(breaks=sequence)
        base <- base + geom_col(position=position_nudge(-.05))
    } else {
        ulx <- part.max(dset, years, parts$key)
        base <- ggplot(data.frame(x=c(0,ulx)), aes(x)) +
            ggtitle(title) +
            xlab(parts$key$desc) +
            ylab(parts$tgt$desc) +
            scale_colour_discrete(name="Year")
        for (year in years) {
            approx <- create.lookup(dset, year=year, tgt.in.key=tgt.in.key)
            base <- base + stat_function(fun=approx, aes_(colour=as.factor(year)))
        }
    }
    print(base)
  }

  main <- function(args=NULL) {
    dset <- doparser(name="looking",
                     need=c("ifile",
                       "maxhh", "supp", "bracket",
                       "binsize",
                       "graphics", "gfile", "gopts",
                       "height", "width",
                       "years",
                       "tgt.in.key", "delta"
                       ),
                     args)
    ggp.ecdf(dset, years=attr(dset, "years"),
             tgt.in.key=attr(dset, "tgt.in.key"), delta=attr(dset, "delta"))
    endup(dset)
    return(0)
  }

  options(error=recover)
  options(warn=2)

  if (!interactive()) {
    rval <- main()
    if (length(warnings()) != 0) {
      print(warnings())
    }
  }
#+END_SRC

**** create.tiles
#+name: create.tiles
#+BEGIN_SRC R :tangle bin/create.tiles :shebang "#!/usr/bin/env Rscript" :results none :eval no-export :exports none
  <<warning>>
  <<looking.bits>>
  <<stats.binned>>


  ggp.tiles <- function(dset, years, tgt.in.key, tiles, diff) {
    "compute percentiles over a set of years"
    df <- data.frame()
    for (year in years) {
        iecdf <- create.lookup(dset, year=year, tgt.in.key)
        data <- iecdf(tiles)
        if (diff) {
            data <- diff(c(0, iecdf(tiles)))
        }
        df <- rbind(df, c(year, data))
    }
    colnames(df) <- c("Year", tiles)
    df
  }

  tiles.main <- function(args=NULL) {
    dset <- doparser(name="looking",
                     need=c("ifile",
                       "maxhh", "supp", "bracket",
                       "binsize",
                       "years", "tgt.in.key", "tiles",
                       "diff"
                       ), args)
    rval <- ggp.tiles(dset,
                      years=attr(dset, "years"),
                      tgt.in.key=attr(dset, "tgt.in.key"),
                      tiles=attr(dset, "tiles"),
                      diff=attr(dset, "diff"))
  }

  main <- function(args=NULL) {
    options("width"=1000)                 # so print() doesn't insert linebreaks
    print(tiles.main(args), row.names=FALSE, digits=2)
  }

  <<looking.endbits>>
#+END_SRC

**** lookup

#+name: lookup
#+BEGIN_SRC R :tangle bin/lookup :shebang "#!/usr/bin/env Rscript" :results none :eval no-export :exports none
  <<warning>>
  <<looking.bits>>
  <<stats.binned>>

  lookup <- function(dset, years, tgt.in.key, keys) {
      df <- data.frame()
      for (year in years) {
          ecdf <- create.lookup(dset, year=year, tgt.in.key=tgt.in.key)
          yset <- dset[dset$YEAR==year,]
          for (key in keys) {
              df <- rbind(df, data.frame(year=year, lookup=key, result=ecdf(key)))
          }
      }
      parts <- tgt.in.key(tgt.in.key)
      colnames(df) <- c("year", parts$key$name, parts$tgt$name)
      df
  }


  lookup.main <- function(args=NULL) {
      dset <- doparser(name="lookup",
                       need=c("ifile",
                              "maxhh", "supp", "bracket",
                              "binsize",
                              "tgt.in.key",
                              "years", "keys"),
                       args)
      df <- lookup(dset,
                   years=attr(dset, "years"), 
                   tgt.in.key=attr(dset, "tgt.in.key"),
                   keys=attr(dset, "keys"))
      endup(dset)
      df
  }

  main <- function(args=NULL) {
      print(lookup.main(args), row.names=FALSE)
  }

  <<looking.endbits>>
#+END_SRC

#+BEGIN_SRC sh :tangle bin/lookup-one :shebang "#!/bin/sh" :results none :eval never :exports none
  # like lookup, but only return the value (for scripting ease)
  ./lookup $* | awk 'NR > 1 { print $3 }'
#+END_SRC

**** for interactive use, return desired dset
this bit is just for running interactively, returns the desired dset

run by, e.g.,
: dset <- main(args=c("--ifile", "ipums/cps_00006.1962-2017-binned.csv"))

#+BEGIN_SRC R :shebang "#!/usr/bin/env Rscript" :results none :eval no-export :exports none
  <<warning>>
  <<looking.bits>>
  <<stats.binned>>
  getdset <- function(args=NULL) {
    dset <- doparser(name="get.dset",
                     need=c("ifile",
                       "maxhh", "supp", "bracket",
                       "binsize",
                       "years", "keys"),
                     args)
    endup(dset)
    dset
  }

  <<looking.endbits>>
#+END_SRC

*** most reported incomes
**** reported (awk script)                                         :noexport:

reduce a dataset to have only one entry per reported hhincome/year.
the CPS dataset has multiple, one for each "category" (represented by,
e.g., hwtsupp) at the same income.  each line of the output dataset
consists of three fields
- YEAR the year
- HHINCOME the household income reported
- COUNT the number of times *this* household income was reported
  during this year

#+begin_src awk :tangle bin/reported :shebang "#!/usr/bin/awk -f" :results none :eval no-export :exports none
  <<warning>>
  BEGIN {
      FS = "[,]"
      seen = 0
  }

  NR == 1 {
      hhincome = ""
      year = ""
      for (i = 1; i <= NF; i++) {
          if ($i == "\"YEAR\"") {
              year = i
          }
          if ($i == "\"HHINCOME\"") {
              hhincome = i
          }
      }
      if (year == "") {
          print "didn't find YEAR column in .csv header" > "/dev/stderr"
          exit 1
      }
      if (hhincome == "") {
          print "didn't find HHINCOME column in .csv header" > "/dev/stderr"
          exit 1
      }
      next                        # move on to next record
  }

  function closeyear() {
      # print "closeyear", $0
      if (!headerprinted) {
          printf("\"YEAR\",\"HHINCOME\",\"COUNT\"\n")
          headerprinted = 1
      }
      for (i in households) {
          printf("%s,%s,%s\n", cyear, i, households[i])
      }
      delete households
      seen = 0
  }

  $year != cyear {
      closeyear()
      cyear = $year
  }

  {
      seen++;
      households[$hhincome]++
  }

  END {
      if (seen > 0) {
          closeyear()
      }
  }
#+end_src

**** reported.dset

this is separated out from reported.dset so we can call it outside the
framework.
#+name: reported.dset.read
#+BEGIN_SRC R :results none :eval no-export :exports none
  reported.dset.read <- function(reported, years) {
      dset <- read.csv(reported, header=TRUE)
      dset <- dset[dset$YEAR >= min(years),]  # before 1967, NA data
      dset
  }
#+END_SRC


read in the data set of reported incomes
#+name: reported.dset
#+BEGIN_SRC R :results none :eval no-export :exports none
  <<reported.dset.read>>
  reported.dset <- function(argv) {        # command line args
    dset <- reported.dset.read(argv$reported, argv$years)

    ## now, remember parameters in attributes
    for (a in names(argv)) {
      if (a != "") {
        attr(dset, a) <- argv[[a]]
      }
    }
    dset
  }
#+END_SRC

**** rankboard
***** invvestigaton, thoughts, links
want [[http://www.visualcapitalist.com/the-historical-returns-by-asset-class-over-the-last-decade/][something]] like what financial services people use to show change
in top-performing assets over time.  ([[http://www.visualcapitalist.com/periodic-table-commodity-returns-2017/][here]] it is called a "periodic
table of X"; points at a nice, if slightly confusing, [[http://www.usfunds.com/interactive/the-periodic-table-of-commodity-returns-2017][interactive
chart]].)

how to display?  [[http://www.math.yorku.ca/SCS/Gallery/images/LAoz.gif][time matrix]] (pointed at by [[https://flowingdata.com/2010/01/07/11-ways-to-visualize-changes-over-time-a-guide/][11 Ways to Visualize
Changes Over Time]])

[[https://datatodisplay.com/blog/chart-design/communicating-changes-rank-time/][Communicating changes in rank over time: bumps charts and slopegraphs]]

[[http://blog.plot.ly/post/117105992082/time-series-graphs-eleven-stunning-ways-you-can][Time Series Graphs & Eleven Stunning Ways You Can Use Them]] (but, not
what we looking for!).

[[http://www.sirvizalot.com/2016/03/color-popularity-for-new-cars-2000-2015.html][Using Ranks to Create Bump Charts in Tableau]] (in tableau).  i thought,
"i don't want a bump chart", but maybe i do?

[[http://www.bagoum.com/rankgraph.html]["tier list ranking chart"]]

[[https://www.r-bloggers.com/stacked-bar-charts-in-r/][stacked bar charts in R]]; [[https://www.r-bloggers.com/using-r-barplot-with-ggplot2/][barplots in R]] (with ggplot2); [[http://ggplot2.tidyverse.org/reference/geom_bar.html][in R]]

then, there are barcharts (we want "stacked") in [[https://www.statmethods.net/advgraphs/trellis.html][lattice]].

one wants an interface that provides a dataframe of labels.  to
display labels, a dataframe of background colors, label text
attributes (font, size, style, color).  and, margins (horizontal and
vertical) between grid elements.  the chessboard will be NxM, just as
is the dataframe.

i was thinking of using the interface to [[https://developer.mozilla.org/en-US/docs/Learn/CSS/CSS_layout/Grids][CSS grids]] as a template for
the interface to this, but it doesn't really seem to have that much of
an interface.  but, for borders, margins, etc., maybe might be a good
template to keep in mind.

actually, R's base graphics has "grid", "abline", text.

([[file:///home/minshall/usr/lib/R/x86_64-pc-linux-gnu-library/tidyr/doc/tidy-data.html][very nice intro]] to H Wickham's long versus wide.)

[[http://sphaerula.com/legacy/R/placingTextInPlots.html][placing text in R plots]]; [[https://www.stat.auckland.ac.nz/~paul/R/fonts.html][specifying fonts in R]]

***** code
****** rankboard code
#+name: rankboard
#+begin_src R :exports none :results none :eval none
  compatiblep <- function(a,b) {
      "is the matrix A compatible with B?  i.e., are the number of rows, columns of B an integer multiple of those of A?"

      if ((nrow(b) %% nrow(a)) != 0) {
          return(FALSE)
      } else if ((ncol(b) %% ncol(a)) != 0) {
          return(FALSE)
      } else {
          return(TRUE)
      }
  }

  gchmax <- function(strings, ...) {
      "apply strwidth(), strheight(), to figure out the most amount
   of space (in user coordinates) taken by the strings STRINGS"

      return(c(width=max(strwidth(strings, ...)),
               height=max(strheight(strings, ...))))
  }


  gsetup <- function(labels, ...) {
      "figure out the size we can use on the current graphics device,
  how big the boxes will each be (for an aspect ratio of 1:1), the
  x- and y-offsets from the edge of the graphics device to the array
  of boxes, etc.  we *SHOULD* also figure out any spacing between
  boxes, etc., as requested by the user.  we should also figure out
  if the text fits in the boxes, maybe shrink the font size, or print
  a warning, if not."

      plot.new()                          # so we can get size
      dimxy <- dev.size("px")             # get size of graphics device
      xpx <- dimxy[1];
      ypx <- dimxy[2];
      par(usr=c(0, xpx, 0, ypx))          # set user coordinate space

      # lines(c(0, xpx, xpx, 0, 0), c(0, 0, ypx, ypx, 0)) # XXX debugging

      cellpx <- min(ypx/nrow(labels), xpx/ncol(labels)) # size of a square
      xoffpx <- (xpx - (cellpx*ncol(labels)))/2 # x offset for lower left
      yoffpx <- (ypx - (cellpx*nrow(labels)))/2 # y ditto

      return(list(cellpx=cellpx,          # size of a square
                  xoffpx=xoffpx,          # bottom x offset
                  yoffpx=yoffpx))         # left y offset
      }

  cboarddraw <- function(labels, colors, rbsqpadding, silentshrink,
      ...) {
      "plots MxN LABELS, each with its own COLORS color;
  this returns the cell size, and the (lower left) X
  and Y offsets (in pixels)"

      stopifnot(compatiblep(colors, labels))

      gp <- gsetup(labels=labels, ...)

      cellpx <- gp$cellpx
      xoffpx <- gp$xoffpx
      yoffpx <- gp$yoffpx

      ## what is the size of our canvas?  (probe underlying graphics?)
      ## divide into MxN chunks
      ## for each chunk

      ## given two sets, compute lower, upper points of rectangle
      m <- ncol(labels)
      n <- nrow(labels)
      ## what's going on here?  the tricky bit for me is getting the
      ## coordinates in the same sequence as the background colors.  the
      ## following, using the col() and row() functions -- which i
      ## learned about at https://stackoverflow.com/a/15287206/1527747
      ## -- does it for me (once i figured out exactly the incantation)
      lx <- ((as.vector(col(labels)-1))  *cellpx)+xoffpx
      ly <- ((as.vector(n-row(labels)))  *cellpx)+yoffpx
      ux <- (as.vector(col(labels))      *cellpx)+xoffpx
      uy <- (as.vector((n+1)-row(labels))*cellpx)+yoffpx
      ## now, generate our squares
      rect(lx, ly, ux, uy, asp=1, col=as.vector(colors), ...)

      ## size of labels in boxes
      cex <- par("cex", ...)               # in case we need to change
      lsize <- gchmax(labels, ...)
      ## now, do labels fit?
      if (any(lsize > ((1-rbsqpadding)*cellpx))) {
          if (!silentshrink) {
              warning(sprintf("at least one label is too large (%s) for box size (%f) -- shrinking font", paste(lsize, collapse=","), cellpx))
          }
          ## we're here because either lsize is to big to fit in the box,
          ## or because it doesn't fit in the 
          cex <- min((1-rbsqpadding)*(cellpx/lsize)) # okay, hope this works!
      }
      ## now, label each square
      txs <- lx+((ux-lx)/2)
      tys <- ly+((uy-ly)/2)
      ## draw text
      text(x=txs, y=tys, labels=as.vector(labels), cex=cex, ...)

      return(c(cellpx=cellpx, xoffpx=xoffpx, yoffpx=yoffpx))
  }

  fillincolors <- function(labels, colors, palette, indices, tags) {
      "check correctness of parameters, and produce an appropriately
  shaped and valued COLORS matrix"

      ## enforce constraints
      if (!is.null(colors)) {
          stopifnot(is.null(palette), is.null(indices), is.null(tags))
          return(colors)                  # that was easy
      }
      if (is.null(indices)) {      #if no indices
          if (is.null(tags)) {
              tags <- labels
          } else {
              tags <- as.matrix(tags)
          }
          ## (we sort uni so caller can maybe predict in case they care a
          ## lot about which color maps to which label value)
          uni <- sort(unique(as.vector(tags))) # number of unique labels
          ## generate indices
          indices <- match(tags, uni) # compute them
      } else {
          stopifnot(is.null(tags))    # constraint
      }
      if (is.null(palette)) {             # fill in palette
          palette <- rainbow(max(indices), s=0.5)
      } else {
          stopifnot(length(palette) >= max(indices))
      }
      ## for match: http://r.789695.n4.nabble.com/Associative-array-tp1588978p1593950.html
      colors <- matrix(palette[indices], nrow=nrow(labels))
      return(colors)
  }


  rankboard <- function(labels, # matrix: text to go in boxes
                        colors=NULL, # matrix: color for each box
                        palette=NULL, # matrix: color for each box
                        indices=NULL, # matrix: index for each box into PALETTE
                        tags=NULL, # matrix: unique identifiers to create indices
                        rbsqpadding=0.05, # numeric: % padding around text in squares
                        title="", # title
                        silentshrink, # don't issue warning when labels shrinking font
                        ...) {
      "
  rankboard produces an eponymously-named \"rankboard\", an
  object that looks somewhat like a multi-colored chessboard,
  where typically each column represents a different point in
  time, and the elements (drawn from some underlying, unspecified,
  universe) in a given column are ranked by some unspecified
  metric.  different columns can contain elements from the same
  different subsets from the universe; typically, any given element
  will appear at most once in any column.

  the LABELS matrix is presented with each column sorted by rank;
  colnames are the time points.  the values of LABELS will be
  written in the boxes in the graph produced by rankboard.

  if the COLORS matrix is non-null, it has the same shape as LABELS,
  and each entry specifies a color for the corresponding entry
  in LABELS.

  if COLORS is non-null, PALETTE, INDICES, and TAGS should *not* be
  specified.

  however, in the case COLORS *is* null:

  PALETTE is a vector of colors to use for the background of the
  boxes.

  if PALETTE is null, it is assigned a vector of colors generated
  by rankboard (currently rainbow, s=0.5).

  if the INDICES matrix is non-null, each entry specifies an index
  into PALETTE (the color table).  in this case, TAGS must be null.

  if INDICES is null, and the TAGS matrix is *also* null, the TAGS
  matrix first receives its values from the LABELS matrix.  then,
  an INDICES matrix is computed by assigning to each entry in it a
  value that is common to the value assigned to all other elements
  with the same value in the TAGS matrix, and distinct from all
  other elements with a distinct value in the TAGS matrix.  i.e.,
  the TAGS matrix is used to partition the numbers
  1:length(unique(TAGS)).

  in effecting this partition, the unique values in the TAGS matrix
  are first sorted; then the resulting index of each tag value is
  used to index into the PALETTE vector.

  in summary: if COLORS is non-null, (PALETTE, INDICES, TAGS) must
  all be null; if INDICES is non-null, TAGS must be null.  in all
  cases, (PALETTE, INDICES, TAGS) *may* all be null.

  RBSQPADDING is how much of the sides of each box should be empty
  of (LABELS) text, as a fraction.

  TITLE is the title given to the graph.

  SILENTSHRINK prevents rankboard from issuing a warning when it
  reduces the font size (see par(\"cex\")) in order to fit LABELS
  into the boxes (in case, for example, the using program normally
  runs with options(warn=2)).
  "
      labels <- as.matrix(labels)         # data frame need not apply

      colors <- fillincolors(colors=colors,
                             labels=labels,
                             palette=palette,
                             indices=indices,
                             tags=tags)

      cb <- cboarddraw(labels=labels, colors=colors,
                       rbsqpadding=rbsqpadding, silentshrink=silentshrink, ...)
      ## do title
      title(main=title, ...)
      ## compute a vector of x-locations for columns
      justxs <- (((1:ncol(labels))-1)*cb["cellpx"])+cb["xoffpx"]+(cb["cellpx"]/2)
      ## do axis tick marks
      axis(side=1, at=justxs, labels=colnames(labels), ...)
  }
#+end_src

****** mostreported code

#+name: mostreported
#+BEGIN_SRC R :results none :eval no-export :exports none
  mostreported <- function(rset, years, keep=30) {
      vals <- matrix(ncol=keep, nrow=0)
      freqs <- matrix(ncol=keep, nrow=0)
      for (year in years) {
          yset <- rset[rset$YEAR==year,]        # choose our year
          zz <- yset$HHINCOME[order(yset$COUNT, decreasing=TRUE)]
          fr <- yset$COUNT[order(yset$COUNT, decreasing=TRUE)]/sum(yset$COUNT)
          vals <- rbind(vals, zz[1:keep])
          freqs <- rbind(freqs, fr[1:keep])
      }
      colnames(vals) <- 1:keep
      colnames(freqs) <- 1:keep
      rownames(vals) <- years
      rownames(freqs) <- years
      return(list(vals=vals, freqs=freqs))
  }
#+end_src

#+name: 

#+BEGIN_SRC R :tangle bin/create.mostreported :shebang "#!/usr/bin/env Rscript" :results none :eval no-export :exports none
  <<reported.bits>>
  <<topcodealpha>>
  <<rankboard>>

  reported.main <- function(args=NULL, ...) {
      dset <- doparser(name="reported",
                       need=c("reported", "keep",
                              "graphics", "gfile", "gopts",
                              "height", "width",
                              "silentshrink", "rbsqpadding",
                              "years"),
                       args)
      res <- mostreported(dset, years=attr(dset, "years"), keep=attr(dset, "keep"))
      tags <- t(res$vals)
      vals <- t(topcodealpha(res$vals))
      freqs <- t(res$freqs)*100           # convert to percentage (still << 1!)
      labels <- matrix(sprintf("%s\n(%0.02f%%)", vals, freqs), nrow=nrow(vals))
      colnames(labels) <- colnames(vals)
      rankboard(labels=labels, tags=tags,
                title="Most reported incomes for selected years",
                silentshrink=attr(dset, "silentshrink"),
                rbsqpadding=attr(dset, "rbsqpadding"),
                ...)
      endup(dset)
      res
  }

  main <- function(args=NULL, ...) {
      reported.main(args, ...)
  }

  <<looking.endbits>>
#+end_src

** playing :noexport:
*** [[https://stackoverflow.com/q/33886629/1527747][stackexchange answer]]

remembering Jeremy Kun's post
https://jeremykun.com/2018/03/25/a-parlor-trick-for-set/ on Set helped
me figure the hard part (for me) of this question.  i realized that
diagonals on the board (what bishops move on) have a constant color.
and, so, their Y-intercept (where they hit the Y-axis) will uniquely
determine their color, and adjacent Y values will have different
colors.  for a square at (x,y), the y intercept (since the slope is 1)
will be at Y == (y-x).  since the parity is the same for addition as
for subtraction, and i'm never sure which mod functions (in which
languages) may give a negative result, i use "(x+y) %% 2".

#+begin_src R
  b <- matrix(nrow=8,ncol=8)              # basic board

  colorindex <- (col(b)+row(b))%%2        # parity of the Y-intercept
                                          # for each square
  colors <- c("red", "white")[colorindex+1] # choose colors
  side <- 1/8                               # side of one square
  ux <- col(b)*side                         # upper x values
  lx <- ux-side                             # lower x values
  uy <- row(b)*side                         # upper y
  ly <- uy-side                             # upper y
  plot.new()                                # initialize R graphics
  rect(lx, ly, ux, uy, col=colors, asp=1)   # draw the board
#+end_src

#+RESULTS:

** local variables :noexport:
# Local Variables:
# org-babel-inline-result-wrap: "%s"
# End:

